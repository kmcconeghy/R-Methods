# Heteroskedastic & Cluster Robust Standard Errors  

```{r packages, include=F, echo=F}
require("tidyverse", quietly=T, warn.conflicts = F)
require("Hmisc", quietly=T, warn.conflicts = F)
require("Scotty", quietly=T, warn.conflicts = F)
mu <- markupSpecs$html  
require("sandwich", quietly=T, warn.conflicts = F)
require("lmtest", quietly=T, warn.conflicts = F)
require("boot", quietly=T, warn.conflicts = F)
require("knitr", quietly=T, warn.conflicts = F)
```

## Introduction  

In this chapter we are evaluating R's capability to compute different kinds of standard errors. Like with many things, R has extensive flexibility here but can be daunting when you want a quick option. To bring this down to earth, I lay out the background, provide practical recommendations, user-written commands and benchmark to STATA.  

### Packages to use  
`r mu$session(cite=F)`  

"Scotty" is my own package. "tidyverse" is Wickam et al. general suite of packages/commands to work with R. "Hmisc" is Frank Harrel's miscellaneous commands, many of which are quite useful.  

"sandwich", "lmtest" and "boot" are specifically relevant to this chapter in order to compute various standard errors (SE).  

## Heteroskedascity  
*Heteroskedascity* in this context refers to a collection of random variables where a given sub-population will have different variability compared with others. Variability being variance or some other measure of dispersion. In constrast *homoskedascity* is when variance is constant across these subpopulations (Figure 1). 

```{r testdataHomo, include=T}
#Generate Data  
  x <- runif(500)
  yHomo <- 2*x + rnorm(500)
  yHetero <- 2*x + x*rnorm(500)
  df <- as.data.frame(cbind(x, yHomo, yHetero))

#Scatter and Fitted Line 
ggplot(data=df, aes(x=x, y=yHomo)) + 
  geom_point() +
  geom_smooth(method='lm', se=F) + 
  xlab("X variable") +
  theme_bw()
 
```  
  
**Figure 1.** Example of homoskedascity. Note how data points appear to be randomly scattered around line of best fit, and that the dispersion *appears* constant across the range of X variable.

```{r testdataHetero, include=T}
#Scatter and Fitted Line 
ggplot(data=df, aes(x=x, y=yHetero)) + 
  geom_point() +
  geom_smooth(method='lm', se=F) + 
  xlab("X variable") +
  theme_bw()
```
  
**Figure 2.** Example of heteroskedascity. See how the dispersion appears greater as X increases.  

@angrist2008mostly  

## Clustering  
@Bertrand04howmuch  

## Test data  

The cluster data was obtained from:  
http://www.kellogg.northwestern.edu/faculty/petersen/htm/papers/se/test_data.txt  

```{r getPeterson}
url <- "http://www.kellogg.northwestern.edu/faculty/petersen/htm/papers/se/test_data.txt"
df <- as_tibble(read.table(url))
print("Head of test dataframe")
names(df) <- c("group", "year", "x", "y")
head(df)

```
  This data represents financial information by year on a group of firms. We use this as a benchmark because several other online posts/bloggers compare this data using different specifications and software.@peterson2009
  
The expected results we will recreate are given here:
http://www.kellogg.northwestern.edu/faculty/petersen/htm/papers/se/test_data.htm

## Standard Errors under iid  
```{r lm}
m1 <- lm(y ~ x, data = df)
coeftest(m1)
```

You can compare these results with Table "OLS Coefficients and Standard Errors". R computes the regression coefficients with $( \textbf{X}'\textbf{X})^{-1}\textbf{X}'\textbf{y}$ i.e. the coefficient is a function of X and y.

In a regression framework you compute standard errors by taking the square root of the diagonal elements of the variance-covariance matrix. 

Equation 1. Covariance matrix of the error term $u$  
$E[{uu}'|\textbf{X}] = \mathbf{\Sigma_{u}}$

Equation 2.  
$\mathbf{\Sigma_{u}} = \sigma^2 I_{N}$

Equation 3. Expectation of the variance of $\beta$ conditional on X.  
$\textrm{Var}[\hat{\mathbf{\beta}}|\textbf{X}] = (\textbf{X}'\textbf{X})^{-1}(\textbf{X}' \mathbf{\Sigma_{u}} \textbf{X}) (\textbf{X}'\textbf{X})^{-1}$

Under the assumption of independent and identically distributed errors (homoskedascity), Eq. 3 is simplified to eq. 4 (transpose matrix, using diagonal elements).  

Equation 4. iid assumed
$\textrm{Var}[\hat{\mathbf{\beta}}|\textbf{X}] = \sigma_{u}^{2}(\textbf{X}'\textbf{X})^{-1}$

Assuming $\sigma_u^2$ is fixed but unknown, and equation to esimate $s^2$ can be used:

Equation 5. Standard Error

$s^2 = \frac{\sum_{i=1}^n e_i^2}{n-k}$

Where $e$ are the squared residuals, $n$ is the sample size, and $k$ are the number of regressors. 

With this information the standard errors above can be replicated manually like so:
```{r manualSE}
# get X matrix/predictors
X <- model.matrix(m1)
# number of obs
n <- dim(X)[1]
# n of predictors
k <- dim(X)[2]
# calculate stan errs as in the above
# sq root of diag elements in vcov
se <- sqrt(diag(solve(crossprod(X)) * as.numeric(crossprod(resid(m1))/(n-k))))
se
```

## Heteroskedastic consistent errors in R  

## Cluster robust errors in R  

## Block bootstrapping  

An alternative to computing special variance-covariance matrices is non-parametric  "block" bootstrapping. To do this, you perform a bootstrapping procedure where you sample the group or "block" instead of unit observation. This has been shown to be about as consistent and unbiased as the above sandwich estimators, and may be advantgeous when the number of clusters is small.@Bertrand04howmuch  

### Bootstrap Program  

```{r BootProg}
Boot.ATE <- function (model, treat, R = 250, block = "", df) 
{
  require(boot)
  require(dplyr)
  family <- model$family
  if (block == "") {
    boot.mod <- function(x, i, model, treat) {
      samp.df <- x[i, ]
      samp.glm <- try(glm(model, data = samp.df, family = family))
      if (inherits(samp.glm, "try-error")) {
        coef <- NA
        ate <- NA
        rr <- NA
        c(coef, ate, rr)
      }
      else {
        df2 <- samp.df
        df2[, paste(treat)] = 1
        pred1. <- predict.glm(samp.glm, newdata = df2, 
          type = "response")
        df2[, paste(treat)] = 0
        pred0. <- predict.glm(samp.glm, newdata = df2, 
          type = "response")
        coef <- samp.glm$coefficients[paste0(treat)]
        ate <- mean(pred1.) - mean(pred0.)
        rr <- mean(pred1.)/mean(pred0.)
        c(coef, ate, rr)
      }
    }
    boot.m <- boot(data = df, statistic = boot.mod, R = R, 
      model = model, treat = treat)
  }
  else {
    Groups = unique(df[, paste(block)])
    boot.mod <- function(x, i, model, treat, df, block, 
      iter = 0) {
      block.df <- data.frame(group = x[i])
      names(block.df) = block
      samp.df <- left_join(block.df, df, by = block)
      samp.glm <- try(glm(model, data = samp.df, family = family))
      if (inherits(samp.glm, "try-error")) {
        coef <- NA
        ate <- NA
        rr <- NA
        c(coef, ate, rr)
      }
      else {
        df2 <- samp.df
        df2[, paste(treat)] = 1
        pred1. <- predict.glm(samp.glm, newdata = df2, 
          type = "response")
        df2[, paste(treat)] = 0
        pred0. <- predict.glm(samp.glm, newdata = df2, 
          type = "response")
        coef <- samp.glm$coefficients[paste0(treat)]
        ate <- mean(pred1.) - mean(pred0.)
        rr <- mean(pred1.)/mean(pred0.)
        c(coef, ate, rr)
      }
    }
    boot.m <- boot(data = Groups, statistic = boot.mod, 
      R = R, model = model, treat = treat, df = df, block = block)
  }
  m1.confint <- c(model$coefficients[paste0(treat)], confint(model, 
    treat, level = 0.95))
  coeff = boot.ci(boot.m, index = 1, type = "perc")
  coeff = c(median(boot.m$t[, 1]), coeff$percent[, 4], coeff$percent[, 
    5])
  names(coeff) <- c("Coeff.", "2.5%", "97.5%")
  ate = boot.ci(boot.m, index = 2, type = "perc")
  ate = c(median(boot.m$t[, 2]), ate$percent[, 4], ate$percent[, 
    5])
  names(ate) <- c("ATE", "2.5%", "97.5%")
  rr = boot.ci(boot.m, index = 3, type = "perc")
  rr = c(median(boot.m$t[, 3]), rr$percent[, 4], rr$percent[, 
    5])
  names(rr) <- c("Rr", "2.5%", "97.5%")
  boot.iter = boot.m$t
  res = list(level = 0.95, model_ci = m1.confint, coeff = coeff, 
    ate = ate, rr = rr, boots = boot.iter)
  return(res)
}
```

## Stata comparison  
A full discussion of STATA programming can be seen here:   http://www.kellogg.northwestern.edu/faculty/petersen/htm/papers/se/se_programming.htm  
STATA blog:  
http://www.stata.com/support/faqs/statistics/standard-errors-and-vce-cluster-option/  

Briefly: In Stata one can specify a variance-covariance matrix that is heteroskedastic consistent with the *vce(robust)* option in regression models.  


e.g. robust option in STATA    
```{}
regress y x, vce(robust)
```

A Huber-White variance-covariance matrix can also be computed by some group with the **vce(cluster *group*)** option in regression models.  

e.g. cluster option in STATA  
```{}
regress y x, vce(cluster group)
```

See:  
http://www.stata.com/support/faqs/statistics/standard-errors-and-vce-cluster-option/  

## Acknowledgements  
This chapter is heavily adapted from several StackExchange and other blog posts.
See:  
http://www.richard-bluhm.com/clustered-ses-in-r-and-stata-2/  
https://sites.google.com/site/waynelinchang/r-code  
https://thetarzan.wordpress.com/2011/05/28/heteroskedasticity-robust-and-clustered-standard-errors-in-r/  

## Bibliography  
@R-base  
