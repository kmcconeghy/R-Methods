# Cluster Robust Standard Errors  

```{r packages, include=F, echo=F}
require("tidyverse", quietly=T, warn.conflicts = F)
require("Hmisc", quietly=T, warn.conflicts = F)
require("devtools", quietly=T, warn.conflicts = F)
require("Scotty", quietly=T, warn.conflicts = F)
mu <- markupSpecs$html  
require("sandwich", quietly=T, warn.conflicts = F)
require("lmtest", quietly=T, warn.conflicts = F)
require("boot", quietly=T, warn.conflicts = F)
require("knitr", quietly=T, warn.conflicts = F)
```

## Introduction  

In this chapter we will evaluate how "clustering" affects standard error computations. In many cases where you are evaluating some exposure x on outcome y, the effect of x on y may be unique subgroups within the data. Many classic examples exist, such as n educational intervention, x, is implemented at the school level but the effect is evaluated on individual student outcome y. The schools represent "clusters" and violate the assumptions of standard errors (residuals are independent and identically distributed) if not accounted for. A special case of clustering occurs when you follow multiple individuals across multiple timepoints (i.e. a pre-post study design). In this case, the exposure and outcome is clustered at the person-level. This will be evaluated in a separate chapter. 

### Packages to use   

`r mu$session(cite=F)`  

"Scotty" is my own package. "tidyverse" is Wickam et al. general suite of packages/commands to work with R. "Hmisc" is Frank Harrel's miscellaneous commands, many of which are quite useful. "sandwich" and "lmtest" are specifically relevant to this chapter in order to compute various standard errors (SE).  

## Clustering  

To illustrate how group-clustering can affect residuals, I will construct a test dataset and visually represent the residuals.

## Cluster robust errors in R  

## Block bootstrapping  

  An alternative to computing a special variance-covariance matrix is using a non-parametric "brute-force" method termed block bootstrapping. To do this, you the sample the dataset with replacement by group or "block" instead of individual observation. The parameters are estimated for each sample instance and stored in a new table. Then, you can either compute the parameter moments (mean, variance etc.) using the stored coefficients or if a 95% parameter interval is the ultimate goal one can simply report the ordered percentiles (e.g., 2.5% - 97.5%). Other methods for computing the intervals exist, such as bias-corrected. Whichever you pick, bootstraps are about as unbiased as the above sandwich estimators, and may be advantageous when the number of clusters is small.
  
## Permutation or "Randomization" Test

(@Bertrand04howmuch) 

### Bootstrap Program  

```{r BootProg}
Boot.ATE <- function (model, treat, R = 250, block = "", df) 
{
  require(boot)
  require(dplyr)
  family <- model$family
  if (block == "") {
    boot.mod <- function(x, i, model, treat) {
      samp.df <- x[i, ]
      samp.glm <- try(glm(model, data = samp.df, family = family))
      if (inherits(samp.glm, "try-error")) {
        coef <- NA
        ate <- NA
        rr <- NA
        c(coef, ate, rr)
      }
      else {
        df2 <- samp.df
        df2[, paste(treat)] = 1
        pred1. <- predict.glm(samp.glm, newdata = df2, 
          type = "response")
        df2[, paste(treat)] = 0
        pred0. <- predict.glm(samp.glm, newdata = df2, 
          type = "response")
        coef <- samp.glm$coefficients[paste0(treat)]
        ate <- mean(pred1.) - mean(pred0.)
        rr <- mean(pred1.)/mean(pred0.)
        c(coef, ate, rr)
      }
    }
    boot.m <- boot(data = df, statistic = boot.mod, R = R, 
      model = model, treat = treat)
  }
  else {
    Groups = unique(df[, paste(block)])
    boot.mod <- function(x, i, model, treat, df, block, 
      iter = 0) {
      block.df <- data.frame(group = x[i])
      names(block.df) = block
      samp.df <- left_join(block.df, df, by = block)
      samp.glm <- try(glm(model, data = samp.df, family = family))
      if (inherits(samp.glm, "try-error")) {
        coef <- NA
        ate <- NA
        rr <- NA
        c(coef, ate, rr)
      }
      else {
        df2 <- samp.df
        df2[, paste(treat)] = 1
        pred1. <- predict.glm(samp.glm, newdata = df2, 
          type = "response")
        df2[, paste(treat)] = 0
        pred0. <- predict.glm(samp.glm, newdata = df2, 
          type = "response")
        coef <- samp.glm$coefficients[paste0(treat)]
        ate <- mean(pred1.) - mean(pred0.)
        rr <- mean(pred1.)/mean(pred0.)
        c(coef, ate, rr)
      }
    }
    boot.m <- boot(data = Groups, statistic = boot.mod, 
      R = R, model = model, treat = treat, df = df, block = block)
  }
  m1.confint <- c(model$coefficients[paste0(treat)], confint(model, 
    treat, level = 0.95))
  coeff = boot.ci(boot.m, index = 1, type = "perc")
  coeff = c(median(boot.m$t[, 1]), coeff$percent[, 4], coeff$percent[, 
    5])
  names(coeff) <- c("Coeff.", "2.5%", "97.5%")
  ate = boot.ci(boot.m, index = 2, type = "perc")
  ate = c(median(boot.m$t[, 2]), ate$percent[, 4], ate$percent[, 
    5])
  names(ate) <- c("ATE", "2.5%", "97.5%")
  rr = boot.ci(boot.m, index = 3, type = "perc")
  rr = c(median(boot.m$t[, 3]), rr$percent[, 4], rr$percent[, 
    5])
  names(rr) <- c("Rr", "2.5%", "97.5%")
  boot.iter = boot.m$t
  res = list(level = 0.95, model_ci = m1.confint, coeff = coeff, 
    ate = ate, rr = rr, boots = boot.iter)
  return(res)
  ##############################################
## Testing coefficients in time-series data ##
##############################################

## Load investment equation data:
data(Investment)

# Fit regression model:
fm.inv <- lm(RealInv ~ RealGNP + RealInt, data = Investment)

## Test coefficients using Newey-West HAC estimator with
## user-defined and data-driven bandwidth and with Parzen kernel:
coeftest(fm.inv, df = Inf, vcov = NeweyWest(fm.inv, lag = 4, prewhite = FALSE))
coeftest(fm.inv, df = Inf, vcov = NeweyWest)

parzenHAC <- function(x, ...) kernHAC(x, kernel = "Parzen", prewhite = 2,
  adjust = FALSE, bw = bwNeweyWest, ...)
coeftest(fm.inv, df = Inf, vcov = parzenHAC)

## Time-series visualization:
plot(Investment[, "RealInv"], type = "b", pch = 19, ylab = "Real investment")
lines(ts(fitted(fm.inv), start = 1964), col = 4)

## 3-dimensional visualization:
library(scatterplot3d)
s3d <- scatterplot3d(Investment[,c(5,7,6)],
  type = "b", angle = 65, scale.y = 1, pch = 16)
s3d$plane3d(fm.inv, lty.box = "solid", col = 4)



###########################################################
## Testing and dating structural changes in the presence ##
## of heteroskedasticity and autocorrelation             ##
###########################################################

## Load real interest series:
data(RealInt)

## OLS-based CUSUM test with quadratic spectral kernel HAC estimate:
ocus <- gefp(RealInt ~ 1, fit = lm, vcov = kernHAC)
plot(ocus, aggregate = FALSE)
sctest(ocus)

## supF test with quadratic spectral kernel HAC estimate:
fs <- Fstats(RealInt ~ 1, vcov = kernHAC)
plot(fs)
sctest(fs)

## Breakpoint estimation and confidence intervals
## with quadratic spectral kernel HAC estimate:
bp <- breakpoints(RealInt ~ 1)
confint(bp, vcov = kernHAC)
plot(bp)

## Visualization:
plot(RealInt, ylab = "Real interest rate")
lines(ts(fitted(bp), start = start(RealInt), freq = 4), col = 4)
lines(confint(bp, vcov = kernHAC))
}
```

## Stata comparison  
A full discussion of STATA programming can be seen here:   http://www.kellogg.northwestern.edu/faculty/petersen/htm/papers/se/se_programming.htm  
STATA blog:  
http://www.stata.com/support/faqs/statistics/standard-errors-and-vce-cluster-option/  

Briefly: In Stata one can specify a variance-covariance matrix that is heteroskedastic consistent with the *vce(robust)* option in regression models.  


e.g. robust option in STATA    
```{}
regress y x, vce(robust)
```

A Huber-White variance-covariance matrix can also be computed by some group with the **vce(cluster *group*)** option in regression models.  

e.g. cluster option in STATA  
```{}
regress y x, vce(cluster group)
```

See:  
http://www.stata.com/support/faqs/statistics/standard-errors-and-vce-cluster-option/  

## Acknowledgements  
This chapter is heavily adapted from several StackExchange and other blog posts.  
See:  
http://www.richard-bluhm.com/clustered-ses-in-r-and-stata-2/  
https://sites.google.com/site/waynelinchang/r-code  
https://thetarzan.wordpress.com/2011/05/28/heteroskedasticity-robust-and-clustered-standard-errors-in-r/  
