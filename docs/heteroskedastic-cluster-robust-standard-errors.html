<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>R-methods</title>
  <meta name="description" content="A collection of working papers covering various statistical, analytical or causal inference problems.">
  <meta name="generator" content="bookdown 0.3.16 and GitBook 2.6.7">

  <meta property="og:title" content="R-methods" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://github.com/kmcconeghy/R-Methods" />
  
  <meta property="og:description" content="A collection of working papers covering various statistical, analytical or causal inference problems." />
  <meta name="github-repo" content="/kmcconeghy/R-Methods" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="R-methods" />
  
  <meta name="twitter:description" content="A collection of working papers covering various statistical, analytical or causal inference problems." />
  

<meta name="author" content="Kevin W. McConeghy">


<meta name="date" content="2017-04-01">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="index.html">
<link rel="next" href="ggplot2-examples.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">R-methods</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="heteroskedastic-cluster-robust-standard-errors.html"><a href="heteroskedastic-cluster-robust-standard-errors.html"><i class="fa fa-check"></i><b>1</b> Heteroskedastic &amp; Cluster Robust Standard Errors</a><ul>
<li class="chapter" data-level="1.1" data-path="heteroskedastic-cluster-robust-standard-errors.html"><a href="heteroskedastic-cluster-robust-standard-errors.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1.1" data-path="heteroskedastic-cluster-robust-standard-errors.html"><a href="heteroskedastic-cluster-robust-standard-errors.html#packages-to-use"><i class="fa fa-check"></i><b>1.1.1</b> Packages to use</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="heteroskedastic-cluster-robust-standard-errors.html"><a href="heteroskedastic-cluster-robust-standard-errors.html#heteroskedascity"><i class="fa fa-check"></i><b>1.2</b> Heteroskedascity</a></li>
<li class="chapter" data-level="1.3" data-path="heteroskedastic-cluster-robust-standard-errors.html"><a href="heteroskedastic-cluster-robust-standard-errors.html#test-data"><i class="fa fa-check"></i><b>1.3</b> Test data</a></li>
<li class="chapter" data-level="1.4" data-path="heteroskedastic-cluster-robust-standard-errors.html"><a href="heteroskedastic-cluster-robust-standard-errors.html#regression-parameter-standard-errors-under-iid"><i class="fa fa-check"></i><b>1.4</b> Regression parameter standard errors under iid</a></li>
<li class="chapter" data-level="1.5" data-path="heteroskedastic-cluster-robust-standard-errors.html"><a href="heteroskedastic-cluster-robust-standard-errors.html#white-heteroskedastic-consistent-errors"><i class="fa fa-check"></i><b>1.5</b> “White” heteroskedastic consistent errors</a><ul>
<li class="chapter" data-level="1.5.1" data-path="heteroskedastic-cluster-robust-standard-errors.html"><a href="heteroskedastic-cluster-robust-standard-errors.html#manual-estimator"><i class="fa fa-check"></i><b>1.5.1</b> Manual estimator</a></li>
<li class="chapter" data-level="1.5.2" data-path="heteroskedastic-cluster-robust-standard-errors.html"><a href="heteroskedastic-cluster-robust-standard-errors.html#r-standard-function"><i class="fa fa-check"></i><b>1.5.2</b> R standard function</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="heteroskedastic-cluster-robust-standard-errors.html"><a href="heteroskedastic-cluster-robust-standard-errors.html#clustering"><i class="fa fa-check"></i><b>1.6</b> Clustering</a></li>
<li class="chapter" data-level="1.7" data-path="heteroskedastic-cluster-robust-standard-errors.html"><a href="heteroskedastic-cluster-robust-standard-errors.html#cluster-robust-errors-in-r"><i class="fa fa-check"></i><b>1.7</b> Cluster robust errors in R</a></li>
<li class="chapter" data-level="1.8" data-path="heteroskedastic-cluster-robust-standard-errors.html"><a href="heteroskedastic-cluster-robust-standard-errors.html#block-bootstrapping"><i class="fa fa-check"></i><b>1.8</b> Block bootstrapping</a></li>
<li class="chapter" data-level="1.9" data-path="heteroskedastic-cluster-robust-standard-errors.html"><a href="heteroskedastic-cluster-robust-standard-errors.html#permutation-or-randomization-test"><i class="fa fa-check"></i><b>1.9</b> Permutation or “Randomization” Test</a><ul>
<li class="chapter" data-level="1.9.1" data-path="heteroskedastic-cluster-robust-standard-errors.html"><a href="heteroskedastic-cluster-robust-standard-errors.html#bootstrap-program"><i class="fa fa-check"></i><b>1.9.1</b> Bootstrap Program</a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="heteroskedastic-cluster-robust-standard-errors.html"><a href="heteroskedastic-cluster-robust-standard-errors.html#stata-comparison"><i class="fa fa-check"></i><b>1.10</b> Stata comparison</a></li>
<li class="chapter" data-level="1.11" data-path="heteroskedastic-cluster-robust-standard-errors.html"><a href="heteroskedastic-cluster-robust-standard-errors.html#acknowledgements"><i class="fa fa-check"></i><b>1.11</b> Acknowledgements</a></li>
<li class="chapter" data-level="1.12" data-path="heteroskedastic-cluster-robust-standard-errors.html"><a href="heteroskedastic-cluster-robust-standard-errors.html#bibliography"><i class="fa fa-check"></i><b>1.12</b> Bibliography</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ggplot2-examples.html"><a href="ggplot2-examples.html"><i class="fa fa-check"></i><b>2</b> ggplot2 Examples</a><ul>
<li class="chapter" data-level="2.1" data-path="ggplot2-examples.html"><a href="ggplot2-examples.html#introduction-1"><i class="fa fa-check"></i><b>2.1</b> Introduction</a><ul>
<li class="chapter" data-level="2.1.1" data-path="ggplot2-examples.html"><a href="ggplot2-examples.html#packages-to-use-1"><i class="fa fa-check"></i><b>2.1.1</b> Packages to use</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">R-methods</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="heteroskedastic-cluster-robust-standard-errors" class="section level1">
<h1><span class="header-section-number">Chapter 1</span> Heteroskedastic &amp; Cluster Robust Standard Errors</h1>
<div id="introduction" class="section level2">
<h2><span class="header-section-number">1.1</span> Introduction</h2>
<p>In this chapter we are evaluating R’s capability to compute different kinds of standard errors. Like with many things, R has extensive flexibility here but can be daunting when you want a quick option. To bring this down to earth, I lay out the background, provide practical recommendations, user-written commands and benchmark to STATA.</p>
<div id="packages-to-use" class="section level3">
<h3><span class="header-section-number">1.1.1</span> Packages to use</h3>
<pre>
 R version 3.3.2 (2016-10-31)
 Platform: x86_64-w64-mingw32/x64 (64-bit)
 Running under: Windows 10 x64 (build 14393)
 
 attached base packages:
 [1] stats     graphics  grDevices utils     datasets  base     
 
 other attached packages:
  [1] knitr_1.15.17     boot_1.3-18       lmtest_0.9-35    
  [4] zoo_1.7-14        sandwich_2.3-4    Scotty_0.0.0.9000
  [7] Hmisc_4.0-2       Formula_1.2-1     survival_2.40-1  
 [10] lattice_0.20-34   dplyr_0.5.0       purrr_0.2.2      
 [13] readr_1.1.0       tidyr_0.6.1       tibble_1.2       
 [16] ggplot2_2.2.1     tidyverse_1.1.1  
 </pre>

<p>“Scotty” is my own package. “tidyverse” is Wickam et al. general suite of packages/commands to work with R. “Hmisc” is Frank Harrel’s miscellaneous commands, many of which are quite useful. “sandwich”, “lmtest” and “boot” are specifically relevant to this chapter in order to compute various standard errors (SE).</p>
</div>
</div>
<div id="heteroskedascity" class="section level2">
<h2><span class="header-section-number">1.2</span> Heteroskedascity</h2>
<p><em>Heteroskedascity</em> in this context refers to a random variable where a given subset of a sample will have different variability compared with others. Variability being variance or some other measure of dispersion. In constrast <em>homoskedascity</em> is when variance is constant across these subpopulations (Figure 1).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Generate Data  </span>
  x &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">500</span>)
  yHomo &lt;-<span class="st"> </span><span class="dv">2</span>*x +<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">500</span>)
  yHetero &lt;-<span class="st"> </span><span class="dv">2</span>*x +<span class="st"> </span>x*<span class="kw">rnorm</span>(<span class="dv">500</span>)
  df &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">cbind</span>(x, yHomo, yHetero))

<span class="co">#Scatter and Fitted Line </span>
<span class="kw">ggplot</span>(<span class="dt">data=</span>df, <span class="kw">aes</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>yHomo)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method=</span><span class="st">&#39;lm&#39;</span>, <span class="dt">se=</span>F) +<span class="st"> </span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;X variable&quot;</span>) +
<span class="st">  </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="01-vcovHC_files/figure-html/testdataHomo-1.png" width="672" /></p>
<p><strong>Figure 1.</strong> Example of homoskedascity. Note how data points appear to be randomly scattered around line of best fit, and that the dispersion <em>appears</em> of the points constant across the range of X variable.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Scatter and Fitted Line </span>
<span class="kw">ggplot</span>(<span class="dt">data=</span>df, <span class="kw">aes</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>yHetero)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method=</span><span class="st">&#39;lm&#39;</span>, <span class="dt">se=</span>F) +<span class="st"> </span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;X variable&quot;</span>) +
<span class="st">  </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="01-vcovHC_files/figure-html/testdataHetero-1.png" width="672" /></p>
<p><strong>Figure 2.</strong> Example of heteroskedascity. See how the dispersion of the points appears greater as X increases.</p>
</div>
<div id="test-data" class="section level2">
<h2><span class="header-section-number">1.3</span> Test data</h2>
<p>The cluster data was obtained from:<br />
<a href="http://www.kellogg.northwestern.edu/faculty/petersen/htm/papers/se/test_data.txt" class="uri">http://www.kellogg.northwestern.edu/faculty/petersen/htm/papers/se/test_data.txt</a></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">url &lt;-<span class="st"> &quot;http://www.kellogg.northwestern.edu/faculty/petersen/htm/papers/se/test_data.txt&quot;</span>
df &lt;-<span class="st"> </span><span class="kw">as_tibble</span>(<span class="kw">read.table</span>(url))
<span class="kw">names</span>(df) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;group&quot;</span>, <span class="st">&quot;year&quot;</span>, <span class="st">&quot;x&quot;</span>, <span class="st">&quot;y&quot;</span>)
<span class="kw">head</span>(df)</code></pre></div>
<pre><code>## # A tibble: 6 × 4
##   group  year          x          y
##   &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;
## 1     1     1 -1.1139730  2.2515350
## 2     1     2 -0.0808538  1.2423460
## 3     1     3 -0.2376072 -1.4263760
## 4     1     4 -0.1524857 -1.1093940
## 5     1     5 -0.0014262  0.9146864
## 6     1     6 -1.2127370 -1.4246860</code></pre>
<p>This data represents financial information by year on a group of firms. I use this as a benchmark because several other online posts/bloggers compare this data using different specifications and software.<span class="citation">Petersen (<a href="#ref-peterson2009">2009</a>)</span></p>
<p>The expected results I will recreate are given here: <a href="http://www.kellogg.northwestern.edu/faculty/petersen/htm/papers/se/test_data.htm" class="uri">http://www.kellogg.northwestern.edu/faculty/petersen/htm/papers/se/test_data.htm</a></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">se_results &lt;-<span class="st"> </span><span class="kw">as_tibble</span>(<span class="kw">matrix</span>(<span class="dt">nrow=</span><span class="dv">8</span>,<span class="dt">ncol=</span><span class="dv">6</span>))
<span class="kw">names</span>(se_results) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;method&quot;</span>,<span class="st">&quot;v-cov&quot;</span>, <span class="st">&quot;int&quot;</span>,<span class="st">&quot;x&quot;</span>,<span class="st">&quot;peterson_int&quot;</span>,<span class="st">&quot;peterson_x&quot;</span>)
method &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;lm&quot;</span>,<span class="st">&quot;manual&quot;</span>,<span class="st">&quot;manual&quot;</span>,<span class="st">&quot;HC0&quot;</span>,<span class="st">&quot;HC1&quot;</span>,<span class="st">&quot;HC2&quot;</span>,<span class="st">&quot;HC3&quot;</span>,<span class="st">&quot;HC4&quot;</span>)
type &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;cons&quot;</span>,<span class="st">&quot;cons&quot;</span>,<span class="st">&quot;whitedfc&quot;</span>,<span class="st">&quot;white&quot;</span>,<span class="st">&quot;whitedfc&quot;</span>,<span class="st">&quot;&quot;</span>,<span class="st">&quot;&quot;</span>,<span class="st">&quot;&quot;</span>)
peterson_int=<span class="kw">c</span>(<span class="fl">0.0284</span>,<span class="ot">NA</span>,<span class="fl">0.0284</span>,<span class="ot">NA</span>,<span class="ot">NA</span>,<span class="ot">NA</span>,<span class="ot">NA</span>,<span class="fl">0.0670</span>)
peterson_x=<span class="st">  </span><span class="kw">c</span>(<span class="fl">0.0286</span>,<span class="ot">NA</span>,<span class="fl">0.0284</span>,<span class="ot">NA</span>,<span class="ot">NA</span>,<span class="ot">NA</span>,<span class="ot">NA</span>,<span class="fl">0.0506</span>)

for (i in <span class="dv">1</span>:<span class="kw">nrow</span>(se_results)) {
  se_results[i,<span class="dv">1</span>] &lt;-<span class="st"> </span>method[i]
  se_results[i,<span class="dv">2</span>] &lt;-<span class="st"> </span>type[i]
  se_results[i,<span class="dv">5</span>] &lt;-<span class="st"> </span>peterson_int[i]
  se_results[i,<span class="dv">6</span>] &lt;-<span class="st"> </span>peterson_x[i]
}</code></pre></div>
</div>
<div id="regression-parameter-standard-errors-under-iid" class="section level2">
<h2><span class="header-section-number">1.4</span> Regression parameter standard errors under iid</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m1 &lt;-<span class="st"> </span><span class="kw">lm</span>(y ~<span class="st"> </span>x, <span class="dt">data =</span> df)
se_results[<span class="dv">1</span>,<span class="dv">3</span>] &lt;-<span class="st"> </span><span class="kw">coeftest</span>(m1)[<span class="dv">1</span>,<span class="dv">2</span>]
se_results[<span class="dv">1</span>,<span class="dv">4</span>] &lt;-<span class="st"> </span><span class="kw">coeftest</span>(m1)[<span class="dv">2</span>,<span class="dv">2</span>]
se_results</code></pre></div>
<pre><code>## # A tibble: 8 × 6
##   method  `v-cov`        int          x peterson_int peterson_x
##    &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;
## 1     lm     cons 0.02835932 0.02858329       0.0284     0.0286
## 2 manual     cons         NA         NA           NA         NA
## 3 manual whitedfc         NA         NA       0.0284     0.0284
## 4    HC0    white         NA         NA           NA         NA
## 5    HC1 whitedfc         NA         NA           NA         NA
## 6    HC2                  NA         NA           NA         NA
## 7    HC3                  NA         NA           NA         NA
## 8    HC4                  NA         NA       0.0670     0.0506</code></pre>
<p>You can compare these results with the first table “OLS Coefficients and Standard Errors” in the Peterson link above. R computes the regression coefficients with the standard <span class="math inline">\((\textbf{X}&#39;\textbf{X})^{-1}\textbf{X}&#39;\textbf{y}\)</span> i.e. the coefficient is a function of X and y.</p>
<p>In a regression framework you compute standard errors by taking the square root of the diagonal elements of the variance-covariance matrix.</p>
<p>Equation 1. Covariance matrix of the error term <span class="math inline">\(u\)</span><br />
<span class="math inline">\(E[{uu}&#39;|\textbf{X}] = \mathbf{\Sigma_{u}}\)</span></p>
<p>Equation 2.<br />
<span class="math inline">\(\mathbf{\Sigma_{u}} = \sigma^2 I_{N}\)</span></p>
<p>Equation 3. Expectation of the variance of <span class="math inline">\(\beta\)</span> conditional on X.<br />
<span class="math inline">\(\textrm{Var}[\hat{\mathbf{\beta}}|\textbf{X}] = (\textbf{X}&#39;\textbf{X})^{-1}(\textbf{X}&#39; \mathbf{\Sigma_{u}} \textbf{X}) (\textbf{X}&#39;\textbf{X})^{-1}\)</span></p>
<p>Under the assumption of independent and identically distributed errors (homoskedascity), Eq. 3 is simplified to eq. 4 (transpose matrix, using diagonal elements).</p>
<p>Equation 4. iid assumed <span class="math inline">\(\textrm{Var}[\hat{\mathbf{\beta}}|\textbf{X}] = \sigma_{u}^{2}(\textbf{X}&#39;\textbf{X})^{-1}\)</span></p>
<p>Assuming <span class="math inline">\(\sigma_u^2\)</span> is fixed but unknown, a given random sample’s variance, <span class="math inline">\(s^2\)</span>, can be estimated:</p>
<p>Equation 5. Standard Error</p>
<p><span class="math inline">\(s^2 = \frac{\sum_{i=1}^n e_i^2}{n-k}\)</span></p>
<p>Where <span class="math inline">\(e\)</span> are the squared residuals, <span class="math inline">\(n\)</span> is the sample size, and <span class="math inline">\(k\)</span> are the number of regressors.</p>
<p>With this information the standard errors above can be replicated manually like so:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">X &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(m1) <span class="co"># get X matrix/predictors</span>
n &lt;-<span class="st"> </span><span class="kw">dim</span>(X)[<span class="dv">1</span>] <span class="co"># number of obs</span>
k &lt;-<span class="st"> </span><span class="kw">dim</span>(X)[<span class="dv">2</span>] <span class="co"># n of predictors</span>

<span class="co"># calculate stan errs as eq in the above</span>
<span class="co"># sq root of diag elements in vcov</span>
se &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">solve</span>(<span class="kw">crossprod</span>(X)) *<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">crossprod</span>(<span class="kw">resid</span>(m1))/(n-k))))
se_results[<span class="dv">2</span>,<span class="dv">3</span>] &lt;-<span class="st"> </span>se[<span class="dv">1</span>]
se_results[<span class="dv">2</span>,<span class="dv">4</span>] &lt;-<span class="st"> </span>se[<span class="dv">2</span>]
se_results</code></pre></div>
<pre><code>## # A tibble: 8 × 6
##   method  `v-cov`        int          x peterson_int peterson_x
##    &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;
## 1     lm     cons 0.02835932 0.02858329       0.0284     0.0286
## 2 manual     cons 0.02835932 0.02858329           NA         NA
## 3 manual whitedfc         NA         NA       0.0284     0.0284
## 4    HC0    white         NA         NA           NA         NA
## 5    HC1 whitedfc         NA         NA           NA         NA
## 6    HC2                  NA         NA           NA         NA
## 7    HC3                  NA         NA           NA         NA
## 8    HC4                  NA         NA       0.0670     0.0506</code></pre>
</div>
<div id="white-heteroskedastic-consistent-errors" class="section level2">
<h2><span class="header-section-number">1.5</span> “White” heteroskedastic consistent errors</h2>
<p>In the setting of heteroskedascity, the parameters are consistent but inefficient and also the variance-covariance matrix is inconsistent (i.e. biased).(<span class="citation">White (<a href="#ref-white1980">1980</a>)</span>) The assumption of the residuals <span class="math inline">\(u\)</span> being <em>identically</em> distributed does not hold, and the diagonal matrix is invalid. However, an alternative variance-covariance matrix can be computed which is heteroskedastic consistent.(<span class="citation">White (<a href="#ref-white1980">1980</a>)</span>)</p>
<p>With the “robust” approach proposed by White et al., you assume the variance of the residual is estimated as a diagonal matrix of each squared residual (vs. average above with <span class="math inline">\(s^2\)</span>). Each j-th row-column element is <span class="math inline">\(\hat{u}_{j}^{2}\)</span> in the diagonal terms of <span class="math inline">\({\Sigma_{u}}\)</span>.</p>
<p>The full equation is:</p>
<div id="manual-estimator" class="section level3">
<h3><span class="header-section-number">1.5.1</span> Manual estimator</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">u &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">resid</span>(m1)) <span class="co"># residual vector</span>
meat1 &lt;-<span class="st"> </span><span class="kw">t</span>(X) %*%<span class="st"> </span><span class="kw">diag</span>(<span class="kw">diag</span>(<span class="kw">crossprod</span>(<span class="kw">t</span>(u)))) %*%<span class="st"> </span>X <span class="co"># Sigma is a diagonal with u^2 as elements</span>
dfc &lt;-<span class="st"> </span>n/(n-k) <span class="co"># degrees of freedom adjust  </span>
se &lt;-<span class="st"> </span><span class="kw">sqrt</span>(dfc*<span class="kw">diag</span>(<span class="kw">solve</span>(<span class="kw">crossprod</span>(X)) %*%<span class="st"> </span>meat1 %*%<span class="st"> </span><span class="kw">solve</span>(<span class="kw">crossprod</span>(X))))
se_results[<span class="dv">3</span>,<span class="dv">3</span>] &lt;-<span class="st"> </span>se[<span class="dv">1</span>]
se_results[<span class="dv">3</span>,<span class="dv">4</span>] &lt;-<span class="st"> </span>se[<span class="dv">2</span>]
se_results</code></pre></div>
<pre><code>## # A tibble: 8 × 6
##   method  `v-cov`        int          x peterson_int peterson_x
##    &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;
## 1     lm     cons 0.02835932 0.02858329       0.0284     0.0286
## 2 manual     cons 0.02835932 0.02858329           NA         NA
## 3 manual whitedfc 0.02836067 0.02839516       0.0284     0.0284
## 4    HC0    white         NA         NA           NA         NA
## 5    HC1 whitedfc         NA         NA           NA         NA
## 6    HC2                  NA         NA           NA         NA
## 7    HC3                  NA         NA           NA         NA
## 8    HC4                  NA         NA       0.0670     0.0506</code></pre>
<p>You will find these “White” or robust standard errors are consistent with the second Peterson table.<span class="citation">(Petersen <a href="#ref-peterson2009">2009</a>)</span> They are also consistent with STATA’s <em>robust</em> option. It is not technically the same as the White paper because STATA does a degree of freedom adjustment for small sample size.</p>
</div>
<div id="r-standard-function" class="section level3">
<h3><span class="header-section-number">1.5.2</span> R standard function</h3>
<p>Using the already written commands you can specify “White” standard errors with the vcovHC function in the sandwich package.<span class="citation">(Zeileis <a href="#ref-Zeileis2006">2006</a>)</span> You can report correct standard errors like below with vcovHC option in function coeftest.</p>
<p>vcovHC has several types available. The general formula for the var-cov matrix is: <span class="math inline">\((X&#39;X)^{-1} X&#39; Omega X (X&#39;X)^{-1}\)</span>.</p>
<p>The specification of <span class="math inline">\(Omega\)</span> is determined by the <code>type=</code> option.</p>
<p><code>type=&quot;cons&quot;</code> <span class="math inline">\(\omega_i = \sigma^2\)</span> Constant variance<br />
<code>type=HC0</code> <span class="math inline">\(\omega_i = \mu^2_i\)</span> the White variance-covariance matrix<br />
<code>type=HC1</code> <span class="math inline">\(\omega_i = \frac{n}{n-k}\mu^2_i\)</span> Small sample correction (STATA).<br />
<code>type=HC2</code> <span class="math inline">\(\omega_i = \frac{\mu^2_i}{1-h_i}\)</span><br />
<code>type=HC3</code> <span class="math inline">\(\omega_i = \frac{\mu^2_i}{(1-h_i)^{2}}\)</span><br />
<code>type=HC4</code> <span class="math inline">\(\omega_i = \frac{\mu^2_i}{(1-h_i)^{\delta_i}}\)</span></p>
<p>Where <span class="math inline">\(h_i = H_{ii}\)</span> are the diagonal elements of the hat matrix and <span class="math inline">\(\delta_i = min({4 }, {h_i}{h¯})\)</span>. The documentation for the sandwich package recommends HC4 based on recent literature.<span class="citation">(“Cribari-Neto F” <a href="#ref-Cribari2004">2004</a>)</span></p>
<pre><code>## Different variance-covariance options with vcovHC</code></pre>
<pre><code>## type = cons</code></pre>
<pre><code>## 
## t test of coefficients:
## 
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 0.029680   0.028359  1.0466   0.2954    
## x           1.034833   0.028583 36.2041   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre><code>## type = HC0</code></pre>
<pre><code>## 
## t test of coefficients:
## 
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 0.029680   0.028355  1.0467   0.2953    
## x           1.034833   0.028389 36.4513   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre><code>## type = HC1</code></pre>
<pre><code>## 
## t test of coefficients:
## 
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 0.029680   0.028361  1.0465   0.2954    
## x           1.034833   0.028395 36.4440   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre><code>## type = HC2,</code></pre>
<pre><code>## 
## t test of coefficients:
## 
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 0.029680   0.028361  1.0465   0.2954    
## x           1.034833   0.028401 36.4368   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre><code>## type = HC3,</code></pre>
<pre><code>## 
## t test of coefficients:
## 
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 0.029680   0.028366  1.0463   0.2955    
## x           1.034833   0.028412 36.4223   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre><code>## type = HC4,</code></pre>
<pre><code>## 
## t test of coefficients:
## 
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 0.029680   0.028363  1.0464   0.2954    
## x           1.034833   0.028418 36.4150   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Lifehack: Rather than use the <code>coeftest</code> function you can also directly modify the standard errors in the regression summary object.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">s &lt;-<span class="st"> </span><span class="kw">summary</span>(m1)
s$coefficients[, <span class="dv">2</span>] &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">diag</span>(<span class="kw">vcovHC</span>(m1, <span class="dt">type=</span><span class="st">&quot;HC1&quot;</span>)))
s</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x, data = df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.7611 -1.3680 -0.0166  1.3387  8.6779 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.02968    0.02836   1.047    0.295    
## x            1.03483    0.02840  36.204   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.005 on 4998 degrees of freedom
## Multiple R-squared:  0.2078, Adjusted R-squared:  0.2076 
## F-statistic:  1311 on 1 and 4998 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<div id="clustering" class="section level2">
<h2><span class="header-section-number">1.6</span> Clustering</h2>
</div>
<div id="cluster-robust-errors-in-r" class="section level2">
<h2><span class="header-section-number">1.7</span> Cluster robust errors in R</h2>
</div>
<div id="block-bootstrapping" class="section level2">
<h2><span class="header-section-number">1.8</span> Block bootstrapping</h2>
<p>An alternative to computing a special variance-covariance matrix is using a non-parametric “brute-force” method termed block bootstrapping. To do this, you the sample the dataset with replacement by group or “block” instead of individual observation. The parameters are estimated for each sample instance and stored in a new table. Then, you can either compute the parameter moments (mean, variance etc.) using the stored coefficients or if a 95% parameter interval is the ultimate goal one can simply report the ordered percentiles (e.g., 2.5% - 97.5%). Other methods for computing the intervals exist, such as bias-corrected. Whichever you pick, bootstraps are about as unbiased as the above sandwich estimators, and may be advantageous when the number of clusters is small.</p>
</div>
<div id="permutation-or-randomization-test" class="section level2">
<h2><span class="header-section-number">1.9</span> Permutation or “Randomization” Test</h2>
<ol class="example" style="list-style-type: decimal">
<li></li>
</ol>
<div id="bootstrap-program" class="section level3">
<h3><span class="header-section-number">1.9.1</span> Bootstrap Program</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Boot.ATE &lt;-<span class="st"> </span>function (model, treat, <span class="dt">R =</span> <span class="dv">250</span>, <span class="dt">block =</span> <span class="st">&quot;&quot;</span>, df) 
{
  <span class="kw">require</span>(boot)
  <span class="kw">require</span>(dplyr)
  family &lt;-<span class="st"> </span>model$family
  if (block ==<span class="st"> &quot;&quot;</span>) {
    boot.mod &lt;-<span class="st"> </span>function(x, i, model, treat) {
      samp.df &lt;-<span class="st"> </span>x[i, ]
      samp.glm &lt;-<span class="st"> </span><span class="kw">try</span>(<span class="kw">glm</span>(model, <span class="dt">data =</span> samp.df, <span class="dt">family =</span> family))
      if (<span class="kw">inherits</span>(samp.glm, <span class="st">&quot;try-error&quot;</span>)) {
        coef &lt;-<span class="st"> </span><span class="ot">NA</span>
        ate &lt;-<span class="st"> </span><span class="ot">NA</span>
        rr &lt;-<span class="st"> </span><span class="ot">NA</span>
        <span class="kw">c</span>(coef, ate, rr)
      }
      else {
        df2 &lt;-<span class="st"> </span>samp.df
        df2[, <span class="kw">paste</span>(treat)] =<span class="st"> </span><span class="dv">1</span>
        pred1. &lt;-<span class="st"> </span><span class="kw">predict.glm</span>(samp.glm, <span class="dt">newdata =</span> df2, 
          <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)
        df2[, <span class="kw">paste</span>(treat)] =<span class="st"> </span><span class="dv">0</span>
        pred0. &lt;-<span class="st"> </span><span class="kw">predict.glm</span>(samp.glm, <span class="dt">newdata =</span> df2, 
          <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)
        coef &lt;-<span class="st"> </span>samp.glm$coefficients[<span class="kw">paste0</span>(treat)]
        ate &lt;-<span class="st"> </span><span class="kw">mean</span>(pred1.) -<span class="st"> </span><span class="kw">mean</span>(pred0.)
        rr &lt;-<span class="st"> </span><span class="kw">mean</span>(pred1.)/<span class="kw">mean</span>(pred0.)
        <span class="kw">c</span>(coef, ate, rr)
      }
    }
    boot.m &lt;-<span class="st"> </span><span class="kw">boot</span>(<span class="dt">data =</span> df, <span class="dt">statistic =</span> boot.mod, <span class="dt">R =</span> R, 
      <span class="dt">model =</span> model, <span class="dt">treat =</span> treat)
  }
  else {
    Groups =<span class="st"> </span><span class="kw">unique</span>(df[, <span class="kw">paste</span>(block)])
    boot.mod &lt;-<span class="st"> </span>function(x, i, model, treat, df, block, 
      <span class="dt">iter =</span> <span class="dv">0</span>) {
      block.df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">group =</span> x[i])
      <span class="kw">names</span>(block.df) =<span class="st"> </span>block
      samp.df &lt;-<span class="st"> </span><span class="kw">left_join</span>(block.df, df, <span class="dt">by =</span> block)
      samp.glm &lt;-<span class="st"> </span><span class="kw">try</span>(<span class="kw">glm</span>(model, <span class="dt">data =</span> samp.df, <span class="dt">family =</span> family))
      if (<span class="kw">inherits</span>(samp.glm, <span class="st">&quot;try-error&quot;</span>)) {
        coef &lt;-<span class="st"> </span><span class="ot">NA</span>
        ate &lt;-<span class="st"> </span><span class="ot">NA</span>
        rr &lt;-<span class="st"> </span><span class="ot">NA</span>
        <span class="kw">c</span>(coef, ate, rr)
      }
      else {
        df2 &lt;-<span class="st"> </span>samp.df
        df2[, <span class="kw">paste</span>(treat)] =<span class="st"> </span><span class="dv">1</span>
        pred1. &lt;-<span class="st"> </span><span class="kw">predict.glm</span>(samp.glm, <span class="dt">newdata =</span> df2, 
          <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)
        df2[, <span class="kw">paste</span>(treat)] =<span class="st"> </span><span class="dv">0</span>
        pred0. &lt;-<span class="st"> </span><span class="kw">predict.glm</span>(samp.glm, <span class="dt">newdata =</span> df2, 
          <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)
        coef &lt;-<span class="st"> </span>samp.glm$coefficients[<span class="kw">paste0</span>(treat)]
        ate &lt;-<span class="st"> </span><span class="kw">mean</span>(pred1.) -<span class="st"> </span><span class="kw">mean</span>(pred0.)
        rr &lt;-<span class="st"> </span><span class="kw">mean</span>(pred1.)/<span class="kw">mean</span>(pred0.)
        <span class="kw">c</span>(coef, ate, rr)
      }
    }
    boot.m &lt;-<span class="st"> </span><span class="kw">boot</span>(<span class="dt">data =</span> Groups, <span class="dt">statistic =</span> boot.mod, 
      <span class="dt">R =</span> R, <span class="dt">model =</span> model, <span class="dt">treat =</span> treat, <span class="dt">df =</span> df, <span class="dt">block =</span> block)
  }
  m1.confint &lt;-<span class="st"> </span><span class="kw">c</span>(model$coefficients[<span class="kw">paste0</span>(treat)], <span class="kw">confint</span>(model, 
    treat, <span class="dt">level =</span> <span class="fl">0.95</span>))
  coeff =<span class="st"> </span><span class="kw">boot.ci</span>(boot.m, <span class="dt">index =</span> <span class="dv">1</span>, <span class="dt">type =</span> <span class="st">&quot;perc&quot;</span>)
  coeff =<span class="st"> </span><span class="kw">c</span>(<span class="kw">median</span>(boot.m$t[, <span class="dv">1</span>]), coeff$percent[, <span class="dv">4</span>], coeff$percent[, 
    <span class="dv">5</span>])
  <span class="kw">names</span>(coeff) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Coeff.&quot;</span>, <span class="st">&quot;2.5%&quot;</span>, <span class="st">&quot;97.5%&quot;</span>)
  ate =<span class="st"> </span><span class="kw">boot.ci</span>(boot.m, <span class="dt">index =</span> <span class="dv">2</span>, <span class="dt">type =</span> <span class="st">&quot;perc&quot;</span>)
  ate =<span class="st"> </span><span class="kw">c</span>(<span class="kw">median</span>(boot.m$t[, <span class="dv">2</span>]), ate$percent[, <span class="dv">4</span>], ate$percent[, 
    <span class="dv">5</span>])
  <span class="kw">names</span>(ate) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;ATE&quot;</span>, <span class="st">&quot;2.5%&quot;</span>, <span class="st">&quot;97.5%&quot;</span>)
  rr =<span class="st"> </span><span class="kw">boot.ci</span>(boot.m, <span class="dt">index =</span> <span class="dv">3</span>, <span class="dt">type =</span> <span class="st">&quot;perc&quot;</span>)
  rr =<span class="st"> </span><span class="kw">c</span>(<span class="kw">median</span>(boot.m$t[, <span class="dv">3</span>]), rr$percent[, <span class="dv">4</span>], rr$percent[, 
    <span class="dv">5</span>])
  <span class="kw">names</span>(rr) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Rr&quot;</span>, <span class="st">&quot;2.5%&quot;</span>, <span class="st">&quot;97.5%&quot;</span>)
  boot.iter =<span class="st"> </span>boot.m$t
  res =<span class="st"> </span><span class="kw">list</span>(<span class="dt">level =</span> <span class="fl">0.95</span>, <span class="dt">model_ci =</span> m1.confint, <span class="dt">coeff =</span> coeff, 
    <span class="dt">ate =</span> ate, <span class="dt">rr =</span> rr, <span class="dt">boots =</span> boot.iter)
  <span class="kw">return</span>(res)
}</code></pre></div>
</div>
</div>
<div id="stata-comparison" class="section level2">
<h2><span class="header-section-number">1.10</span> Stata comparison</h2>
<p>A full discussion of STATA programming can be seen here: <a href="http://www.kellogg.northwestern.edu/faculty/petersen/htm/papers/se/se_programming.htm" class="uri">http://www.kellogg.northwestern.edu/faculty/petersen/htm/papers/se/se_programming.htm</a><br />
STATA blog:<br />
<a href="http://www.stata.com/support/faqs/statistics/standard-errors-and-vce-cluster-option/" class="uri">http://www.stata.com/support/faqs/statistics/standard-errors-and-vce-cluster-option/</a></p>
<p>Briefly: In Stata one can specify a variance-covariance matrix that is heteroskedastic consistent with the <em>vce(robust)</em> option in regression models.</p>
<p>e.g. robust option in STATA</p>
<pre><code>regress y x, vce(robust)</code></pre>
<p>A Huber-White variance-covariance matrix can also be computed by some group with the <strong>vce(cluster <em>group</em>)</strong> option in regression models.</p>
<p>e.g. cluster option in STATA</p>
<pre><code>regress y x, vce(cluster group)</code></pre>
<p>See:<br />
<a href="http://www.stata.com/support/faqs/statistics/standard-errors-and-vce-cluster-option/" class="uri">http://www.stata.com/support/faqs/statistics/standard-errors-and-vce-cluster-option/</a></p>
</div>
<div id="acknowledgements" class="section level2">
<h2><span class="header-section-number">1.11</span> Acknowledgements</h2>
<p>This chapter is heavily adapted from several StackExchange and other blog posts. See:<br />
<a href="http://www.richard-bluhm.com/clustered-ses-in-r-and-stata-2/" class="uri">http://www.richard-bluhm.com/clustered-ses-in-r-and-stata-2/</a><br />
<a href="https://sites.google.com/site/waynelinchang/r-code" class="uri">https://sites.google.com/site/waynelinchang/r-code</a><br />
<a href="https://thetarzan.wordpress.com/2011/05/28/heteroskedasticity-robust-and-clustered-standard-errors-in-r/" class="uri">https://thetarzan.wordpress.com/2011/05/28/heteroskedasticity-robust-and-clustered-standard-errors-in-r/</a></p>
</div>
<div id="bibliography" class="section level2">
<h2><span class="header-section-number">1.12</span> Bibliography</h2>
<p><span class="citation">R Core Team (<a href="#ref-R-base">2016</a>)</span><br />
<span class="citation">Angrist and Pischke (<a href="#ref-angrist2008mostly">2008</a>)</span></p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-peterson2009">
<p>Petersen, MA. 2009. “Estimating Standard Errors in Finance Panel Data Sets: Comparing Approaches.” <em>Review of Financial Studies</em> 22: 435–80.</p>
</div>
<div id="ref-white1980">
<p>White, Halbert. 1980. “A Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct Test for Heteroskedasticity.” <em>Econometrika</em> 48 (4): 817–88.</p>
</div>
<div id="ref-Zeileis2006">
<p>Zeileis, Achim. 2006. “Object-Oriented Computation of Sandwich Estimators.” <em>Journal of Statistical Software</em> 16 (9): 1–16. <a href="http://www.jstatsoft.org/v16/i09/." class="uri">http://www.jstatsoft.org/v16/i09/.</a></p>
</div>
<div id="ref-Cribari2004">
<p>“Cribari-Neto F.” 2004. <em>Computational Statistics &amp; Data Analysis</em> 45.</p>
</div>
<div id="ref-R-base">
<p>R Core Team. 2016. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="https://www.R-project.org/" class="uri">https://www.R-project.org/</a>.</p>
</div>
<div id="ref-angrist2008mostly">
<p>Angrist, J.D., and J.S. Pischke. 2008. <em>Mostly Harmless Econometrics: An Empiricist’s Companion</em>. Princeton University Press. <a href="https://books.google.com/books?id=ztXL21Xd8v8C" class="uri">https://books.google.com/books?id=ztXL21Xd8v8C</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ggplot2-examples.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
