[
["index.html", "R-methods Preface", " R-methods Kevin W. McConeghy 2017-04-01 Preface "],
["heteroskedastic-cluster-robust-standard-errors.html", "Chapter 1 Heteroskedastic &amp; Cluster Robust Standard Errors 1.1 Introduction 1.2 Heteroskedascity 1.3 Test data 1.4 Regression parameter standard errors under iid 1.5 “White” heteroskedastic consistent errors 1.6 Clustering 1.7 Cluster robust errors in R 1.8 Block bootstrapping 1.9 Permutation or “Randomization” Test 1.10 Stata comparison 1.11 Acknowledgements 1.12 Bibliography", " Chapter 1 Heteroskedastic &amp; Cluster Robust Standard Errors 1.1 Introduction In this chapter we are evaluating R’s capability to compute different kinds of standard errors. Like with many things, R has extensive flexibility here but can be daunting when you want a quick option. To bring this down to earth, I lay out the background, provide practical recommendations, user-written commands and benchmark to STATA. 1.1.1 Packages to use R version 3.3.2 (2016-10-31) Platform: x86_64-w64-mingw32/x64 (64-bit) Running under: Windows 10 x64 (build 14393) attached base packages: [1] stats graphics grDevices utils datasets base other attached packages: [1] knitr_1.15.17 boot_1.3-18 lmtest_0.9-35 [4] zoo_1.7-14 sandwich_2.3-4 Scotty_0.0.0.9000 [7] Hmisc_4.0-2 Formula_1.2-1 survival_2.40-1 [10] lattice_0.20-34 dplyr_0.5.0 purrr_0.2.2 [13] readr_1.1.0 tidyr_0.6.1 tibble_1.2 [16] ggplot2_2.2.1 tidyverse_1.1.1 “Scotty” is my own package. “tidyverse” is Wickam et al. general suite of packages/commands to work with R. “Hmisc” is Frank Harrel’s miscellaneous commands, many of which are quite useful. “sandwich”, “lmtest” and “boot” are specifically relevant to this chapter in order to compute various standard errors (SE). 1.2 Heteroskedascity Heteroskedascity in this context refers to a collection of random variables where a given sub-population will have different variability compared with others. Variability being variance or some other measure of dispersion. In constrast homoskedascity is when variance is constant across these subpopulations (Figure 1). #Generate Data x &lt;- runif(500) yHomo &lt;- 2*x + rnorm(500) yHetero &lt;- 2*x + x*rnorm(500) df &lt;- as.data.frame(cbind(x, yHomo, yHetero)) #Scatter and Fitted Line ggplot(data=df, aes(x=x, y=yHomo)) + geom_point() + geom_smooth(method=&#39;lm&#39;, se=F) + xlab(&quot;X variable&quot;) + theme_bw() Figure 1. Example of homoskedascity. Note how data points appear to be randomly scattered around line of best fit, and that the dispersion appears constant across the range of X variable. #Scatter and Fitted Line ggplot(data=df, aes(x=x, y=yHetero)) + geom_point() + geom_smooth(method=&#39;lm&#39;, se=F) + xlab(&quot;X variable&quot;) + theme_bw() Figure 2. Example of heteroskedascity. See how the dispersion appears greater as X increases. 1.3 Test data The cluster data was obtained from: http://www.kellogg.northwestern.edu/faculty/petersen/htm/papers/se/test_data.txt url &lt;- &quot;http://www.kellogg.northwestern.edu/faculty/petersen/htm/papers/se/test_data.txt&quot; df &lt;- as_tibble(read.table(url)) print(&quot;Head of test dataframe&quot;) ## [1] &quot;Head of test dataframe&quot; names(df) &lt;- c(&quot;group&quot;, &quot;year&quot;, &quot;x&quot;, &quot;y&quot;) head(df) ## # A tibble: 6 × 4 ## group year x y ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 -1.1139730 2.2515350 ## 2 1 2 -0.0808538 1.2423460 ## 3 1 3 -0.2376072 -1.4263760 ## 4 1 4 -0.1524857 -1.1093940 ## 5 1 5 -0.0014262 0.9146864 ## 6 1 6 -1.2127370 -1.4246860 This data represents financial information by year on a group of firms. We use this as a benchmark because several other online posts/bloggers compare this data using different specifications and software.Petersen (2009) The expected results we will recreate are given here: http://www.kellogg.northwestern.edu/faculty/petersen/htm/papers/se/test_data.htm 1.4 Regression parameter standard errors under iid m1 &lt;- lm(y ~ x, data = df) coeftest(m1) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.029680 0.028359 1.0466 0.2954 ## x 1.034833 0.028583 36.2041 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 You can compare these results with first table “OLS Coefficients and Standard Errors” with the Peterson link above. R computes the regression coefficients with \\((\\textbf{X}&#39;\\textbf{X})^{-1}\\textbf{X}&#39;\\textbf{y}\\) i.e. the coefficient is a function of X and y. In a regression framework you compute standard errors by taking the square root of the diagonal elements of the variance-covariance matrix. Equation 1. Covariance matrix of the error term \\(u\\) \\(E[{uu}&#39;|\\textbf{X}] = \\mathbf{\\Sigma_{u}}\\) Equation 2. \\(\\mathbf{\\Sigma_{u}} = \\sigma^2 I_{N}\\) Equation 3. Expectation of the variance of \\(\\beta\\) conditional on X. \\(\\textrm{Var}[\\hat{\\mathbf{\\beta}}|\\textbf{X}] = (\\textbf{X}&#39;\\textbf{X})^{-1}(\\textbf{X}&#39; \\mathbf{\\Sigma_{u}} \\textbf{X}) (\\textbf{X}&#39;\\textbf{X})^{-1}\\) Under the assumption of independent and identically distributed errors (homoskedascity), Eq. 3 is simplified to eq. 4 (transpose matrix, using diagonal elements). Equation 4. iid assumed \\(\\textrm{Var}[\\hat{\\mathbf{\\beta}}|\\textbf{X}] = \\sigma_{u}^{2}(\\textbf{X}&#39;\\textbf{X})^{-1}\\) Assuming \\(\\sigma_u^2\\) is fixed but unknown, this equation to esimate \\(s^2\\) can be used: Equation 5. Standard Error \\(s^2 = \\frac{\\sum_{i=1}^n e_i^2}{n-k}\\) Where \\(e\\) are the squared residuals, \\(n\\) is the sample size, and \\(k\\) are the number of regressors. With this information the standard errors above can be replicated manually like so: X &lt;- model.matrix(m1) # get X matrix/predictors n &lt;- dim(X)[1] # number of obs k &lt;- dim(X)[2] # n of predictors # calculate stan errs as eq in the above # sq root of diag elements in vcov se &lt;- sqrt(diag(solve(crossprod(X)) * as.numeric(crossprod(resid(m1))/(n-k)))) se ## (Intercept) x ## 0.02835932 0.02858329 1.5 “White” heteroskedastic consistent errors In the setting of heteroskedascity, the parameters are consistent but inefficient and also the variance-covariance matrix is inconsistent (i.e. biased).(H (1980)) The assumption of the residuals \\(u\\) being identically distributed does not hold, and the diagonal matrix is invalid. However, an alternative variance-covariance matrix can be computed which is heteroskedastic consistent.(H (1980)) With the “robust” approach proposed by White et al., you assume the variance of the residual is estimated as a diagonal matrix of each squared residual (vs. average above with \\(s^2\\)). Each j-th row-column element is \\(\\hat{u}_{j}^{2}\\) in the diagonal terms of \\({\\Sigma_{u}}\\). The full equation is: 1.5.1 Manual estimator u &lt;- matrix(resid(m1)) # residual vector meat1 &lt;- t(X) %*% diag(diag(crossprod(t(u)))) %*% X # Sigma is a diagonal with u^2 as elements dfc &lt;- n/(n-k) # degrees of freedom adjust se &lt;- sqrt(dfc*diag(solve(crossprod(X)) %*% meat1 %*% solve(crossprod(X)))) se ## (Intercept) x ## 0.02836067 0.02839516 You will find these “White” or robust standard errors are consistent with the second Peterson table.Petersen (2009) They are also consistent with STATA’s robust option. It is not technically the same as the White paper, but it includes a degree of freedom adjustment. 1.5.2 R standard function Using the already written commands you can specific “White” standard errors by specifying the vcovHC function in the sandwich package.(Zeileis (2006)) You can report correct standard errors like below with vcovHC option in function coeftest. vcovHC has several types available. The general formula for the var-cov matrix is: \\((X&#39;X)^{-1} X&#39; Omega X (X&#39;X)^{-1}\\). The specification of \\(Omega\\) is determined by the type= option. HC2 : ωi = uˆ 2 i 1 − hi HC3 : ωi = uˆ 2 i (1 − hi) 2 HC4 : ωi = uˆ 2 i (1 − hi) δi type=&quot;cons&quot; \\(\\omega_i = \\sigma^2\\) Constant variance type=HC0 \\(\\omega_i = \\mu^2_i\\) the White variance-covariance matrix type=HC1 \\(\\omega_i = \\frac{n}{n-k}\\mu^2_i\\) Small sample correction (STATA). type=HC2 \\(\\omega_i = \\frac{\\mu^2_i}{1-h_i}\\) type=HC3 \\(\\omega_i = \\frac{\\mu^2_i}{(1-h_i)^{2}}\\) type=HC4 \\(\\omega_i = \\frac{\\mu^2_i}{(1-h_i)^{\\delta_i}}\\) where \\(h_i = H_{ii}\\) are the diagonal elements of the hat matrix and \\(\\delta_i = min({4 }, {h_i}{h¯})\\). The documentation for the sandwich package recommends HC4 based on recent literature.((“Cribari-Neto F” 2004)) type=HC0 is the “White” calculation. Lifehack: Rather than use the coeftest function you can also directly modify the standard errors in the regression summary object. s &lt;- summary(m1) s$coefficients[, 2] &lt;- sqrt(diag(vcovHC(m1, type=&quot;HC1&quot;))) s ## ## Call: ## lm(formula = y ~ x, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.7611 -1.3680 -0.0166 1.3387 8.6779 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.02968 0.02836 1.047 0.295 ## x 1.03483 0.02840 36.204 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.005 on 4998 degrees of freedom ## Multiple R-squared: 0.2078, Adjusted R-squared: 0.2076 ## F-statistic: 1311 on 1 and 4998 DF, p-value: &lt; 2.2e-16 1.6 Clustering 1 1.7 Cluster robust errors in R 1.8 Block bootstrapping An alternative to computing a special variance-covariance matrix is using a non-parametric “brute-force” method termed block bootstrapping. To do this, you the sample the dataset with replacement by group or “block” instead of individual observation. The parameters are estimated for each sample instance and stored in a new table. Then, you can either compute the parameter moments (mean, variance etc.) using the stored coefficients or if a 95% parameter interval is the ultimate goal one can simple report the ordered percentiles (e.g., 2.5% - 97.5%). Other methods for computing the intervals exist, such as bias-corrected. Whichever you pick, bootstraps are about as unbiased as the above sandwich estimators, and may be advantageous when the number of clusters is small. 1.9 Permutation or “Randomization” Test 1.9.1 Bootstrap Program Boot.ATE &lt;- function (model, treat, R = 250, block = &quot;&quot;, df) { require(boot) require(dplyr) family &lt;- model$family if (block == &quot;&quot;) { boot.mod &lt;- function(x, i, model, treat) { samp.df &lt;- x[i, ] samp.glm &lt;- try(glm(model, data = samp.df, family = family)) if (inherits(samp.glm, &quot;try-error&quot;)) { coef &lt;- NA ate &lt;- NA rr &lt;- NA c(coef, ate, rr) } else { df2 &lt;- samp.df df2[, paste(treat)] = 1 pred1. &lt;- predict.glm(samp.glm, newdata = df2, type = &quot;response&quot;) df2[, paste(treat)] = 0 pred0. &lt;- predict.glm(samp.glm, newdata = df2, type = &quot;response&quot;) coef &lt;- samp.glm$coefficients[paste0(treat)] ate &lt;- mean(pred1.) - mean(pred0.) rr &lt;- mean(pred1.)/mean(pred0.) c(coef, ate, rr) } } boot.m &lt;- boot(data = df, statistic = boot.mod, R = R, model = model, treat = treat) } else { Groups = unique(df[, paste(block)]) boot.mod &lt;- function(x, i, model, treat, df, block, iter = 0) { block.df &lt;- data.frame(group = x[i]) names(block.df) = block samp.df &lt;- left_join(block.df, df, by = block) samp.glm &lt;- try(glm(model, data = samp.df, family = family)) if (inherits(samp.glm, &quot;try-error&quot;)) { coef &lt;- NA ate &lt;- NA rr &lt;- NA c(coef, ate, rr) } else { df2 &lt;- samp.df df2[, paste(treat)] = 1 pred1. &lt;- predict.glm(samp.glm, newdata = df2, type = &quot;response&quot;) df2[, paste(treat)] = 0 pred0. &lt;- predict.glm(samp.glm, newdata = df2, type = &quot;response&quot;) coef &lt;- samp.glm$coefficients[paste0(treat)] ate &lt;- mean(pred1.) - mean(pred0.) rr &lt;- mean(pred1.)/mean(pred0.) c(coef, ate, rr) } } boot.m &lt;- boot(data = Groups, statistic = boot.mod, R = R, model = model, treat = treat, df = df, block = block) } m1.confint &lt;- c(model$coefficients[paste0(treat)], confint(model, treat, level = 0.95)) coeff = boot.ci(boot.m, index = 1, type = &quot;perc&quot;) coeff = c(median(boot.m$t[, 1]), coeff$percent[, 4], coeff$percent[, 5]) names(coeff) &lt;- c(&quot;Coeff.&quot;, &quot;2.5%&quot;, &quot;97.5%&quot;) ate = boot.ci(boot.m, index = 2, type = &quot;perc&quot;) ate = c(median(boot.m$t[, 2]), ate$percent[, 4], ate$percent[, 5]) names(ate) &lt;- c(&quot;ATE&quot;, &quot;2.5%&quot;, &quot;97.5%&quot;) rr = boot.ci(boot.m, index = 3, type = &quot;perc&quot;) rr = c(median(boot.m$t[, 3]), rr$percent[, 4], rr$percent[, 5]) names(rr) &lt;- c(&quot;Rr&quot;, &quot;2.5%&quot;, &quot;97.5%&quot;) boot.iter = boot.m$t res = list(level = 0.95, model_ci = m1.confint, coeff = coeff, ate = ate, rr = rr, boots = boot.iter) return(res) } 1.10 Stata comparison A full discussion of STATA programming can be seen here: http://www.kellogg.northwestern.edu/faculty/petersen/htm/papers/se/se_programming.htm STATA blog: http://www.stata.com/support/faqs/statistics/standard-errors-and-vce-cluster-option/ Briefly: In Stata one can specify a variance-covariance matrix that is heteroskedastic consistent with the vce(robust) option in regression models. e.g. robust option in STATA regress y x, vce(robust) A Huber-White variance-covariance matrix can also be computed by some group with the vce(cluster group) option in regression models. e.g. cluster option in STATA regress y x, vce(cluster group) See: http://www.stata.com/support/faqs/statistics/standard-errors-and-vce-cluster-option/ 1.11 Acknowledgements This chapter is heavily adapted from several StackExchange and other blog posts. See: http://www.richard-bluhm.com/clustered-ses-in-r-and-stata-2/ https://sites.google.com/site/waynelinchang/r-code https://thetarzan.wordpress.com/2011/05/28/heteroskedasticity-robust-and-clustered-standard-errors-in-r/ 1.12 Bibliography R Core Team (2016) Angrist and Pischke (2008) References "],
["ggplot2-examples.html", "Chapter 2 ggplot2 Examples 2.1 Introduction", " Chapter 2 ggplot2 Examples 2.1 Introduction In this chapter we are evaluating R’s capability to compute different kinds of standard errors. Like with many things, R has extensive flexibility here but can be daunting when you want a quick option. To bring this flexibility down to earth, I lay out the background, provide practical recommendations, user-written commands and benchmark to STATA. 2.1.1 Packages to use R version 3.3.2 (2016-10-31) Platform: x86_64-w64-mingw32/x64 (64-bit) Running under: Windows 10 x64 (build 14393) attached base packages: [1] stats graphics grDevices utils datasets base other attached packages: [1] Scotty_0.0.0.9000 Hmisc_4.0-2 Formula_1.2-1 [4] survival_2.40-1 lattice_0.20-34 dplyr_0.5.0 [7] purrr_0.2.2 readr_1.1.0 tidyr_0.6.1 [10] tibble_1.2 ggplot2_2.2.1 tidyverse_1.1.1 “Scotty” is my own package. “tidyverse” is Wickam et al. general suite of packages/commands to work with R. “Hmisc” is Frank Harrel’s miscellaneous commands, many of which are quite useful. ## Acknowledgements This chapter is heavily adapted from several StackExchange and other blog posts. See: R Core Team (2016) "]
]
