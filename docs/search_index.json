[
["index.html", "R-methods Chapter 1 Preface 1.1 Introduction", " R-methods Kevin W. McConeghy 2017-04-09 Chapter 1 Preface 1.1 Introduction This book contains a collection of working papers covering various statistical, analytical or causal inference problems using R statistical software.(R Core Team 2016) It is a work-in-progress and meant as a personal reference tool, but freely open to the public. Enjoy! Depending on the chapter many packages may be loaded into the environment. “Scotty” is my own package many of the functions are discussed in the text. “tidyverse” is Wickam et al. general suite of packages/commands to work with R. “Hmisc” is Frank Harrel’s miscellaneous commands, many of which are quite useful. Other packages may be loaded as needed for a specific chapter. My knowledge in pharmacology, pharmacoepidemiology, causal inference, and econometrics comes primarily from graduate work, ongoing research and personal interest.(Angrist and Pischke 2008) References "],
["creating-longitudinal-datasets-from-individual-records.html", "Chapter 2 Creating Longitudinal Datasets From Individual Records 2.1 Introduction 2.2 Construct dataset 2.3 Show Data 2.4 Simple group counts 2.5 Cohort entries by year counts 2.6 Incidence 2.7 Prevalence 2.8 Incidence Density 2.9 Tie everything together", " Chapter 2 Creating Longitudinal Datasets From Individual Records 2.1 Introduction 2.1.1 Packages to use R version 3.3.2 (2016-10-31) Platform: x86_64-w64-mingw32/x64 (64-bit) Running under: Windows 10 x64 (build 14393) attached base packages: [1] methods stats graphics grDevices utils datasets base other attached packages: [1] gridExtra_2.2.1 lubridate_1.6.0 knitr_1.15.17 [4] Scotty_0.0.0.9000 devtools_1.12.0 Hmisc_4.0-2 [7] Formula_1.2-1 survival_2.41-3 lattice_0.20-34 [10] dplyr_0.5.0 purrr_0.2.2 readr_1.1.0 [13] tidyr_0.6.1 tibble_1.3.0 ggplot2_2.2.1 [16] tidyverse_1.1.1 I often come across the following issue in my work: Sometimes you are working with a dataset where each row is a nursing home assessment, admission record or some other per person observation. However, perhaps you are more interesting in analyzing group-level changes over time. In order to do this, you need to reshape and summarize these individual records into counts in a “panel” dataset. In this new dataset structure I want each row to be a unique time- group- summary of the data. Some extensions of this include computing incidence (no. events per 1000 persons) and incidence density (no. events per 1000 person-years) measures. I will go through some examples and show how these datasets and measures can be constructed from person or observation unit-level data, assuming you had cohort entry-dates (i.e. admission), event dates (the date of some thing you wish to quantify) and stop-dates (i.e. discharge). 2.2 Construct dataset First I will construct a test dataset to use in this chapter and subsequent ones. In this hypothetical I take a group of admissions that starts counting on 2000/01/01. Each entry will have a random exit up to 1000 days from entry, but censored at 2009/12/31 (because in my hypothetical example this is my study endpoint). Each entry will then have a random group classification (state), and event (0 or 1). The event will be assumed one per admission (E.g. death..). I didn’t allow for multiple admissions by person. I then generate the event as drawn from a random bernoulli distribution with probability \\(p_g\\) Where \\(g\\) is a group-specific effect randomly generated from a uniform distribution between 0 and 0.3. set.seed(12345) #So you get same result sampsize = 20000 df &lt;- data.frame(id=1:sampsize, CohortEntry = sample(seq(as.Date(&#39;2000/01/01&#39;), as.Date(&#39;2009/12/31&#39;), by=&#39;day&#39;), replace=T, sampsize)) #CohortIn, CohortOut, Group df &lt;- df %&gt;% mutate(CohortExit = CohortEntry+sample(0:365,sampsize, replace=T)) %&gt;% #CohortExit Date (up to 1000 days from start) mutate(State = factor(sample(state.name, sampsize, replace=T))) #Random state for each group df$CohortExit &lt;- as.Date(sapply(df$CohortExit, function(x) X = min(x,as.Date(&#39;2009/12/31&#39;))), origin=origin) #Censor at &#39;study end&#39; #Group Effect State &lt;- as.data.frame(state.name) State$Effect &lt;- runif(50, min=0, max=0.3) #Random effect by group #Generate random event by group effect getReffect &lt;- function(df, group) { p &lt;- df[df$&#39;state.name&#39;==group,&quot;Effect&quot;] event = as.integer(rbernoulli(1, p = p)) return(event) } df$Event &lt;- sapply(df$id, function(x) getReffect(State, df$State[x])) #Generate random event #For more complicated procedures below, assign random event date between cohort start with event=0 to NA randomDate &lt;- function(TimeIn, TimeOut, Event) { RDate &lt;- sample(TimeIn:TimeOut, 1, replace=T) RDate &lt;- ifelse(Event==0,NA,RDate) return(RDate) } df$EventDate = sapply(df$id, function(x) randomDate(df$CohortEntry[x], df$CohortExit[x], df$Event[x])) df$EventDate = as.Date(df$EventDate, origin=origin) kable(head(df, n=10), align=c(&#39;c&#39;)) id CohortEntry CohortExit State Event EventDate 1 2007-03-18 2007-10-10 Ohio 0 NA 2 2008-10-04 2009-10-04 Rhode Island 0 NA 3 2007-08-11 2008-04-24 Ohio 0 NA 4 2008-11-11 2009-10-26 Nevada 0 NA 5 2004-07-25 2004-08-29 New York 0 NA 6 2001-08-30 2002-05-29 Alaska 1 2002-03-23 7 2003-04-02 2004-02-23 North Carolina 0 NA 8 2005-02-03 2005-10-02 Maine 0 NA 9 2007-04-12 2007-06-09 Nebraska 0 NA 10 2009-11-24 2009-12-31 Maine 0 NA 2.3 Show Data #Scatter and Fitted Line p1 &lt;- ggplot(data=df, aes(x=CohortEntry)) + geom_histogram(aes(y = ..density..), binwidth = 4, fill=I(&quot;blue&quot;), alpha=I(0.4)) + geom_density(col=2) + xlab(&quot;Cohort Entry&quot;) + theme_bw() p2 &lt;- ggplot(data=df, aes(x=CohortExit)) + geom_histogram(aes(y = ..density..), binwidth = 4, fill=I(&quot;blue&quot;), alpha=I(0.4)) + geom_density(col=2) + xlab(&quot;Cohort Exit&quot;) + theme_bw() grid.arrange(p1, p2, nrow=1) A simple, even distribution to work with (cohort exit is even except for censored at study end date). 2.4 Simple group counts If you simply seek to count the total no. of records by groups this is simple. dfState &lt;- df %&gt;% group_by(State) %&gt;% #Tells dplyr to create grouped object, and then execute following at that unit summarise(Records = n()) #count individuals cat(&quot;Counts of Records by State&quot;) ## Counts of Records by State kable(head(dfState, n=10), align=c(&#39;c&#39;)) State Records Alabama 396 Alaska 393 Arizona 393 Arkansas 385 California 372 Colorado 375 Connecticut 401 Delaware 399 Florida 376 Georgia 401 Also, if you wish to see the quantity of some event, this is easy also: 2.4.1 Simple Event Counts dfState &lt;- df %&gt;% group_by(State) %&gt;% #Tells dplyr to create grouped object, and then execute following at that unit summarise(Events = sum(Event)) #count no of events cat(&quot;Counts of Events by State&quot;) ## Counts of Events by State kable(head(dfState, n=10), align=c(&#39;c&#39;)) State Events Alabama 46 Alaska 26 Arizona 19 Arkansas 3 California 5 Colorado 84 Connecticut 61 Delaware 120 Florida 113 Georgia 59 Note how the number of events is quite different by group, because we specified this above. If you wish to count records by time, this is still pretty easy, but you have to be more specific. For example, if I want to count the number of cohort entries by year, this is how: 2.5 Cohort entries by year counts #First make year var df &lt;- df %&gt;% mutate(EntryYear = year(CohortEntry)) #year function from lubridate #Second group by this var dfGroup &lt;- df %&gt;% group_by(EntryYear) %&gt;% summarise(Records = n(), Events = sum(Event)) #count individuals cat(&quot;Counts of Records by Cohort Entry Year&quot;) ## Counts of Records by Cohort Entry Year kable(head(dfGroup, n=10), align=c(&#39;c&#39;)) EntryYear Records Events 2000 2015 274 2001 1987 273 2002 2041 288 2003 1933 295 2004 2043 269 2005 2068 313 2006 1964 285 2007 2019 308 2008 1974 297 2009 1956 293 Here you can see the number who enter the cohort by year, and among those how many events are observed. 2.6 Incidence The next step will get a little trickier. Let’s say we aren’t interested in how many individuals entered/exited the cohort in a given year as above. Rather we want to identify how many patients are in the cohort during a specified period of time (e.g. year). This is the “population at risk”. We wish to estimate the the number of events / population at risk. This is incidence. So we need to take the “CohortEntry”, “CohortExit” date variables and compute how many individuals were in the cohort in year 1, year 2 etc. What makes this tricky is that individuals don’t start and stop at the same time and cross multiple time units (years in this case). Here is one method where I compute the no. of persons in the cohort in a given year, “incident” events and event rate: #Create New Dataframe by Time Unit TimeMin &lt;- min(year(df$CohortEntry)) #lowest time unit observed TimeMax &lt;- max(year(df$CohortExit)) #highest time unit observed #This following sequence step is good, in case a certain year was skipped #(i.e. no admits that year) dfTime &lt;- TimeMin:TimeMax %&gt;% #Sequence years as_tibble() %&gt;% mutate(x2 = NA, x3 = NA) names(dfTime) &lt;- c(&quot;Year&quot;, &quot;Residents&quot;, &quot;Inc. Events&quot;) #Write a time-interval program for Residence (assuming x is year) InCohort &lt;- function(x, TimeIn, TimeOut) { #Note that the following line works because of R vectorization count &lt;- if_else(x&gt;=year(TimeIn) &amp; x&lt;=year(TimeOut),1,0) #Test if x is TimeIn&lt;=x&lt;=TimeOut InCohortN &lt;- sum(count) #Add up total people return(InCohortN) #return } #Write a time-interval program for Event IncEvent &lt;- function(x, Event, EventDate) { #Note that the following line works because of R vectorization events &lt;- if_else(Event==1 &amp; x==year(EventDate),1,0) #Added condition of event==1 InCohortEvents &lt;- sum(events) #Add up total events in that year return(InCohortEvents) #return } dfTime$Residents &lt;- sapply(dfTime$Year, function(x) InCohort(x, df$CohortEntry,df$CohortExit)) dfTime$&#39;Inc. Events&#39; &lt;- sapply(dfTime$Year, function(x) IncEvent(x, df$Event, df$EventDate)) dfTime$&#39;Event Rate&#39; &lt;- dfTime$&#39;Inc. Events&#39; / dfTime$Residents kable(head(dfTime, n=10), align=c(&#39;c&#39;), digits=3) Year Residents Inc. Events Event Rate 2000 2015 200 0.099 2001 2981 271 0.091 2002 3040 284 0.093 2003 2951 291 0.099 2004 3034 283 0.093 2005 3063 299 0.098 2006 2956 295 0.100 2007 2986 301 0.101 2008 2972 294 0.099 2009 2945 373 0.127 I didn’t specify a time-trend in my sample generation, and that is why the event rate is relatively constant over time. We can double-check this worked with the following specific code: #Logic # IF 2004 is less than or equal to EntryDate (i.e. they entered before or during 2004) # AND 2004 is less than or equal to Exit (i.e. they exited after or during 2004) Check &lt;- ifelse(2004&gt;=year(df$CohortEntry) &amp; 2004&lt;=year(df$CohortExit),1,0) cat(&quot;2004 people :&quot;, sum(Check)) ## 2004 people : 3034 2.7 Prevalence The primary difference between incidence and prevalence is that incidence is only counting new cases. In order to compute prevalence, we need to identify individuals still in the cohort in a given year, but having the event in previous years as well. Here’s how: #Code same as above TimeMin &lt;- min(year(df$CohortEntry)) TimeMax &lt;- max(year(df$CohortExit)) dfTime &lt;- TimeMin:TimeMax %&gt;% as_tibble() %&gt;% mutate(x2 = NA, x3 = NA) names(dfTime) &lt;- c(&quot;Year&quot;, &quot;Residents&quot;, &quot;Prev. Events&quot;) InCohort &lt;- function(x, TimeIn, TimeOut) { count &lt;- if_else(x&gt;=year(TimeIn) &amp; x&lt;=year(TimeOut),1,0) InCohortN &lt;- sum(count) return(InCohortN) } #Write a time-interval program for Event PrevEvent &lt;- function(x, TimeIn, TimeOut, Event, EventDate) { #Key difference follows: events &lt;- if_else(x&gt;=year(TimeIn) &amp; x&lt;=year(TimeOut) &amp; Event==1 &amp; x&gt;=year(EventDate),1,0) InCohortEvents &lt;- sum(events) #Add up total events in that year return(InCohortEvents) #return } dfTime$Residents &lt;- sapply(dfTime$Year, function(x) InCohort(x, df$CohortEntry,df$CohortExit)) dfTime$&#39;Prev. Events&#39; &lt;- sapply(dfTime$Year, function(x) PrevEvent(x, df$CohortEntry, df$CohortExit, df$Event, df$EventDate)) dfTime$&#39;Prevalence Rate&#39; &lt;- dfTime$&#39;Prev. Events&#39; / dfTime$Residents kable(head(dfTime, n=10), align=c(&#39;c&#39;), digits=3) Year Residents Prev. Events Prevalence Rate 2000 2015 200 0.099 2001 2981 339 0.114 2002 3040 352 0.116 2003 2951 360 0.122 2004 3034 360 0.119 2005 3063 358 0.117 2006 2956 381 0.129 2007 2986 356 0.119 2008 2972 372 0.125 2009 2945 436 0.148 Note how the prevalence rate is higher, because you are counting events which happened in previous years. Also, the prevalence rate in the first year 2000 is the same as the incidence rate because we have no information before 2000 to add into the prevalent rate. If this is a problem in your empirical research, one approach would be to only count events from 2001 onward, and use the first year as a “lead-in” period. 2.8 Incidence Density The next goal is to report not the number of events per total population in a given time interval, but rather the number of events per person time. This is sometimes called incidence density, often reported as “x events per 100-person years”. Conceptually you are saying you would on average expect x events in 1 person followed for 100 years, or x events in 100 persons followed for one year. This can be very useful when there is differing time accrued by the unit of observation (person, admission etc.). However, the relevance and interpretability of this measure is very specific to the thing being studied, i.e. it may not be a reasonable assumption that the event rate is constant over 100 years! The key new measure here is person-time, heres how to compute it: df &lt;- df %&gt;% mutate(PerTime = as.integer(CohortExit - CohortEntry)) #timeDiff #Second group by this var dfGroup &lt;- df %&gt;% group_by(EntryYear) %&gt;% summarise(Records = n(), &#39;Days in Cohort&#39; = sum(PerTime)) #count individuals kable(head(dfGroup, n=10), align=c(&#39;c&#39;)) EntryYear Records Days in Cohort 2000 2015 366029 2001 1987 361894 2002 2041 366716 2003 1933 354578 2004 2043 366435 2005 2068 365334 2006 1964 355171 2007 2019 369771 2008 1974 364829 2009 1956 236663 That was easy! person- or unit-time accrued is simply CohortExit - CohortEntry. However, note we are reporting person-time by the year individuals entered the cohort, not an actual interval of time. It gets more complicated if you want to report aggregated person-time by some time interval. Because our individuals cross years, some of their person time accrues to one or more years. Here is how to do this: #Code same as above TimeMin &lt;- min(year(df$CohortEntry)) TimeMax &lt;- max(year(df$CohortExit)) dfTime &lt;- TimeMin:TimeMax %&gt;% as_tibble() %&gt;% mutate(x2 = NA, x3 = NA) names(dfTime) &lt;- c(&#39;Year&#39;, &#39;Residents&#39;,&#39;Inc. Events&#39;) InCohort &lt;- function(x, TimeIn, TimeOut) { count &lt;- if_else(x&gt;=year(TimeIn) &amp; x&lt;=year(TimeOut),1,0) InCohortN &lt;- sum(count) return(InCohortN) } dfTime$Residents &lt;- sapply(dfTime$Year, function(x) InCohort(x, df$CohortEntry,df$CohortExit)) #Number of residents IncEvent &lt;- function(x, Event, EventDate) { #Note that the following line works because of R vectorization events &lt;- if_else(Event==1 &amp; x==year(EventDate),1,0) #Added condition of event==1 InCohortEvents &lt;- sum(events) #Add up total events in that year return(InCohortEvents) #return } dfTime$&#39;Inc. Events&#39; &lt;- sapply(dfTime$Year, function(x) IncEvent(x, df$Event, df$EventDate)) #Function to compute person time TimeInCohort &lt;- function(x, TimeIn, TimeOut) { #Key steps here: FirstDay &lt;- as.Date(paste0(&#39;01/01/&#39;,x), format=&#39;%m/%d/%Y&#39;, origin=origin) LastDay &lt;- as.Date(paste0(&#39;12/31/&#39;,x), format=&#39;%m/%d/%Y&#39;, origin=origin) #Compute starting point as either first day of year or TimeIn #if TimeIn is &gt; FirstDay #&#39;&#39; opposite for Year Stop YearStart &lt;- sapply(TimeIn, function(x) max(x, FirstDay)) YearStop &lt;- sapply(TimeOut, function(x) min(x, LastDay)) #Compute DaysInYear, if present in that year DaysInYear &lt;- ifelse(x&gt;=year(TimeIn) &amp; x&lt;=year(TimeOut), YearStop - YearStart, 0) InCohortDays &lt;- sum(DaysInYear) return(InCohortDays) } dfTime$&#39;Person Time&#39; &lt;- sapply(dfTime$Year, function(x) TimeInCohort(x, df$CohortEntry,df$CohortExit)) #Person time per year kable(head(dfTime, n=10), align=c(&#39;c&#39;)) Year Residents Inc. Events Person Time 2000 2015 200 242146 2001 2981 271 363133 2002 3040 284 364779 2003 2951 291 357520 2004 3034 283 360320 2005 3063 299 369744 2006 2956 295 353797 2007 2986 301 365960 2008 2972 294 362423 2009 2945 373 358655 Then from this computing the incidence density rate is trivial, and you can report as some arbitrary quantity (i.e. per 100 years). #Code same as above #Events / Patient Days dfTime$`Incidence Density` &lt;- dfTime$`Inc. Events` / dfTime$`Person Time` #Events / Patients Days * 365 days * 100 years dfTime$`Event rate per 100 person years` &lt;- dfTime$`Incidence Density` * 365 * 100 kable(head(dfTime, n=10), align=c(&#39;c&#39;), digits = 4) Year Residents Inc. Events Person Time Incidence Density Event rate per 100 person years 2000 2015 200 242146 8e-04 30.1471 2001 2981 271 363133 7e-04 27.2393 2002 3040 284 364779 8e-04 28.4172 2003 2951 291 357520 8e-04 29.7088 2004 3034 283 360320 8e-04 28.6676 2005 3063 299 369744 8e-04 29.5164 2006 2956 295 353797 8e-04 30.4341 2007 2986 301 365960 8e-04 30.0210 2008 2972 294 362423 8e-04 29.6090 2009 2945 373 358655 1e-03 37.9599 The intepretation “In 2003, the event rate 29.7 per 100 person-years” or “In 2003, the 1-year risk of an event is 29.7 per 100 persons”. 2.9 Tie everything together Now that the methods are established, a panel dataset can be constructed where each row is a unique group, year. The primary measure will be the incidence density. #Panel parameters YearMin &lt;- min(year(df$CohortEntry)) YearMax &lt;- max(year(df$CohortExit)) years &lt;- seq(YearMin, YearMax) groups &lt;- df$State #Build Panel dfPanel &lt;- unique(as_tibble(expand.grid(groups,years))) names(dfPanel) &lt;- c(&quot;Group&quot;,&quot;Year&quot;) dfPanel &lt;- dfPanel %&gt;% arrange(Group, Year) #Build Matching TimeVars df &lt;- df %&gt;% mutate(YearIn = year(CohortEntry)) %&gt;% mutate(YearOut = year(CohortExit)) #A full function that incorporates all steps MakePanel &lt;- function(Panel, dfid) { Panel &lt;- c(Panel[[1]],as.integer(Panel[[2]])) #Build Panel data dfidpanel &lt;- df %&gt;% filter(State == Panel[[1]], YearIn &lt;= Panel[[2]], YearOut &gt;=Panel[[2]]) #Count Events dfevents&lt;- dfidpanel %&gt;% filter(dfidpanel$State== Panel[[1]] &amp; year(dfidpanel$EventDate)==Panel[[2]]) #Count up days FirstDay &lt;- as.Date(paste0(&#39;01/01/&#39;,Panel[[2]]), format=&#39;%m/%d/%Y&#39;, origin=origin) LastDay &lt;- as.Date(paste0(&#39;12/31/&#39;,Panel[[2]]), format=&#39;%m/%d/%Y&#39;, origin=origin) #Compute starting point as either first day of year or TimeIn #if TimeIn is &gt; FirstDay #&#39;&#39; opposite for Year Stop YearStart &lt;- sapply(dfidpanel$CohortEntry, function(x) max(x, FirstDay)) YearStop &lt;- sapply(dfidpanel$CohortExit, function(x) min(x, LastDay)) #Compute DaysInYear, if present in that year DaysInYear &lt;- ifelse(Panel[[2]]&gt;=year(dfidpanel$CohortEntry) &amp; Panel[[2]]&lt;=year(dfidpanel$CohortExit), YearStop - YearStart, 0) #Build Array NRes &lt;- nrow(dfidpanel) NEvents &lt;- sum(dfevents$Event) #Add up total events in that year NDays &lt;- sum(DaysInYear) PanelAdd &lt;- cbind(NRes, NEvents, NDays) return(PanelAdd) } dfPanel[,3:5] &lt;- t(apply(dfPanel, 1, function(x) MakePanel(x, df))) names(dfPanel) &lt;- c(&quot;Residents&quot;, &quot;Events&quot;, &quot;Resident-Days&quot;) dfPanel$`Event Rate` &lt;- dfPanel$Events / dfPanel$`Resident-Days` * 365 * 100 #This step could take a while #The apply function is pull each panel one at a time, MakePanel fills in extra values and returns row #dfPanel &lt;- sapply(dfPanel, function(x) MakePanel(x, df)) kable(head(dfPanel, n=10), align=c(&#39;c&#39;), digits=4) Residents Events Resident-Days NA NA Event Rate Alabama 2000 39 3 5227 1871795 Alabama 2001 54 7 5331 1352528 Alabama 2002 60 6 7736 1217883 Alabama 2003 58 5 6631 1260509 Alabama 2004 71 6 8628 1030225 Alabama 2005 67 4 7151 1092276 Alabama 2006 62 2 8243 1180952 Alabama 2007 58 1 6893 1263026 Alabama 2008 57 7 7027 1285825 Alabama 2009 55 5 7149 1333245 "],
["heteroskedastic-cluster-robust-standard-errors.html", "Chapter 3 Heteroskedastic &amp; Cluster Robust Standard Errors 3.1 Introduction 3.2 Test data 3.3 Linear Regression Model 3.4 Estimation of regression parameters and variance 3.5 Heteroskedascity 3.6 “White” heteroskedastic consistent errors 3.7 Acknowledgements 3.8 Bibliography", " Chapter 3 Heteroskedastic &amp; Cluster Robust Standard Errors 3.1 Introduction In this chapter we are evaluating R’s capability to compute different kinds of standard errors. Like with many things, R has extensive flexibility here but can be daunting when you want a quick option. To bring this down to earth, I lay out the background, provide practical recommendations, user-written commands and benchmark to STATA. 3.1.1 Packages to use R version 3.3.2 (2016-10-31) Platform: x86_64-w64-mingw32/x64 (64-bit) Running under: Windows 10 x64 (build 14393) attached base packages: [1] stats graphics grDevices utils datasets base other attached packages: [1] knitr_1.15.17 boot_1.3-18 lmtest_0.9-35 [4] zoo_1.7-14 sandwich_2.3-4 Scotty_0.0.0.9000 [7] devtools_1.12.0 Hmisc_4.0-2 Formula_1.2-1 [10] survival_2.41-3 lattice_0.20-34 dplyr_0.5.0 [13] purrr_0.2.2 readr_1.1.0 tidyr_0.6.1 [16] tibble_1.3.0 ggplot2_2.2.1 tidyverse_1.1.1 “Scotty” is my own package. “tidyverse” is Wickam et al. general suite of packages/commands to work with R. “Hmisc” is Frank Harrel’s miscellaneous commands, many of which are quite useful. “sandwich”, “lmtest” and “boot” are specifically relevant to this chapter in order to compute various standard errors (SE). 3.2 Test data To test and demonstrate code and assumptions are correct. I utilize the “PublicSchools” dataset in the “sandwich” package. This dataset is well-described in peer-reviewed research, and standard text books (Table 14.1 in Green [1993]).(Greene 1993, Zeileis (2004), Zeileis (2006), (“Cribari-Neto F” 2004)) The data comes originally from a 1979 report on per capita public school expenditures and per capita income by state from the U.S. Dept. of Commerce. ## Load public schools data, omit NA in Wisconsin, scale income and make squared Income term: data(PublicSchools) df &lt;- as_tibble(PublicSchools %&gt;% na.omit() %&gt;% mutate(Income = Income*1e-04)) %&gt;% mutate(Income2 = Income^2) head(df) ## # A tibble: 6 × 3 ## Expenditure Income Income2 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 275 0.6247 0.3902501 ## 2 821 1.0851 1.1774420 ## 3 339 0.7374 0.5437588 ## 4 275 0.6183 0.3822949 ## 5 387 0.8850 0.7832250 ## 6 452 0.8001 0.6401600 se_results &lt;- as_tibble(matrix(nrow=4,ncol=6)) names(se_results) &lt;- c(&quot;method&quot;,&quot;vcov Matrix&quot;, &quot;Income_Beta&quot;,&quot;Income_SE&quot;,&quot;Income2_Beta&quot;,&quot;Income2_SE&quot;) method &lt;- c(&quot;manual&quot;,&quot;lm&quot;,&quot;manual&quot;,&quot;HC0&quot;) #,&quot;HC1&quot;,&quot;HC2&quot;,&quot;HC3&quot;,&quot;HC4&quot;) type &lt;- c(&quot;iid&quot;,&quot;iid&quot;,&quot;White&quot;,&quot;White (dfc)&quot;) for (i in 1:nrow(se_results)) { se_results[i,1] &lt;- method[i] se_results[i,2] &lt;- type[i] } 3.3 Linear Regression Model First, I start with the classical ordinary least squares framework. \\[y_i = X_i\\beta + u_{i} \\quad \\textrm{where} \\quad i = 1,..,n\\] Where \\(y\\) is a dependent variable, \\(X\\) is a vector of regressors (i.e. independent variables) with \\(k\\)-dimensions,\\(\\beta\\) is a vector of the coefficients for \\(X\\), and \\(u\\) is the residual error term. In matrix notation often simply as: \\(y= X\\beta+u\\). Under normal assumptions, the mean of \\(u_i\\) (that is the residual of a given observation \\(i\\)) should be zero and possess a constant variance across all subsets of \\(i\\). The second assumption is my focus here, which is often incorrect in empirical research. 3.4 Estimation of regression parameters and variance Typically in empirical research you are interested in estimating some or all parameter coefficients, and a measure of variance or precision on that parameter. With this most empiricists will make statement along the lines of “A 1-unit change in \\(x\\) produces a \\(\\beta\\)-unit change in \\(y\\), and a null hypothesis of \\(\\beta\\)=0 is rejected with 95% confidence”. In our example, assume we want to model per capita expenditures regressed on income. First, I demonstrate how to estimate your parameter coefficient, \\(\\beta_1\\) the coefficient on income and the square root of the variance, \\(\\sigma\\). 3.4.1 Manually computed beta parameters R basically computes the regression coefficients with the standard \\((\\textbf{X}&#39;\\textbf{X})^{-1}\\textbf{X}&#39;\\textbf{y}\\) i.e. the coefficient is a function of X and y. You can do this manually like so: Y = as.matrix(df$Expenditure) X = as.matrix(cbind(1,df$Income, df$Income2)) #Add one for intercept beta = solve(t(X) %*% X) %*% (t(X) %*% Y) rownames(beta) &lt;- c(&quot;Int&quot;,&quot;Income&quot;,&quot;Income2&quot;) colnames(beta) &lt;- c(&quot;Beta&quot;) se_results[1,3] &lt;- beta[2,1] se_results[1,5] &lt;- beta[3,1] se_results ## # A tibble: 4 × 6 ## method `vcov Matrix` Income_Beta Income_SE Income2_Beta Income2_SE ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 manual iid -1834.203 NA 1587.042 NA ## 2 lm iid NA NA NA NA ## 3 manual White NA NA NA NA ## 4 HC0 White (dfc) NA NA NA NA 3.4.2 Manually computed standard errors In a regression framework you compute standard errors by taking the square root of the diagonal elements of the variance-covariance matrix. As defined above, consider \\(u\\) is normally distributied with mean=0, and standard deviation, \\(\\sigma^2I\\). Where \\(\\sigma^2\\) is the variance. First, define the expectation of the variance of \\(\\beta\\) conditional on X. \\[\\textrm{Var}[\\hat{\\mathbf{\\beta}}|\\textbf{X}] = (\\textbf{X}&#39;\\textbf{X})^{-1}(\\textbf{X}&#39; \\mathbf{\\sigma^2_{u}}\\mathbf{I}\\textbf{X}) (\\textbf{X}&#39;\\textbf{X})^{-1}\\] If you assume that \\(u\\) is indepedent (i.e. orthogonal) to \\(\\beta\\), and identically distributed across subpopulations of \\(\\beta\\). The variance of a random vector X and non-random matrix = matrix * Var(X) * matrix’, can be expressed as: \\[\\textrm{Var}[\\hat{\\mathbf{\\beta}}|\\textbf{X}] = \\mathbf{\\sigma^2_{u}}(\\textbf{X}&#39;\\textbf{X})^{-1}\\] \\[E[{uu}&#39;|\\textbf{X}] = \\mathbf{\\Sigma_{u}}\\] Assuming \\(\\sigma_u^2\\) is fixed but unknown, a given random sample’s variance, \\(s^2\\), can be estimated: Equation 5. Standard Error \\[s^2 = \\frac{\\sum_{i=1}^n e_i^2}{n-k}\\] Where \\(e\\) are the squared residuals, \\(n\\) is the sample size, and \\(k\\) are the number of regressors. With this information the standard errors above can be replicated manually like so: Y = as.matrix(df$Expenditure) #Dependent variable X = as.matrix(cbind(1,df$Income, df$Income2)) #Design matrix, add one for intercept beta = solve(t(X) %*% X) %*% (t(X) %*% Y) #Solve for beta as above n &lt;- dim(X)[1] # number of obs k &lt;- dim(X)[2] # n of predictors # calculate stan errs as eq in the above SigmaSq &lt;- sum((Y - X%*%beta)^2)/(n-k) # (sum residuals)^2 / (degree of freedom correction) i.e. estimate of sigma-squared vcovMat &lt;- SigmaSq*chol2inv(chol(t(X)%*%X)) # variance covariance matrix StdErr &lt;- sqrt(diag(vcovMat)) #sq root of diagonal se_results[1,4] &lt;- StdErr[2] se_results[1,6] &lt;- StdErr[3] se_results ## # A tibble: 4 × 6 ## method `vcov Matrix` Income_Beta Income_SE Income2_Beta Income2_SE ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 manual iid -1834.203 828.9855 1587.042 519.0768 ## 2 lm iid NA NA NA NA ## 3 manual White NA NA NA NA ## 4 HC0 White (dfc) NA NA NA NA 3.4.3 R lm function To confirm the above we can compute the same with the the lm function m1 &lt;- lm(Expenditure ~ Income + Income2, data = df) se_results[2,3] &lt;- coeftest(m1)[2,1] se_results[2,4] &lt;- coeftest(m1)[2,2] se_results[2,5] &lt;- coeftest(m1)[3,1] se_results[2,6] &lt;- coeftest(m1)[3,2] se_results ## # A tibble: 4 × 6 ## method `vcov Matrix` Income_Beta Income_SE Income2_Beta Income2_SE ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 manual iid -1834.203 828.9855 1587.042 519.0768 ## 2 lm iid -1834.203 828.9855 1587.042 519.0768 ## 3 manual White NA NA NA NA ## 4 HC0 White (dfc) NA NA NA NA The estimates are identical. However the critical assumption here of \\(u\\) being “iid”, can often be wrong in empirical research. In the following, I broadly define these concepts. 3.5 Heteroskedascity Heteroskedascity in this context refers to a random variable where a given subset of a sample will have different variability compared with others. Variability being variance or some other measure of dispersion. In constrast homoskedascity is when variance is constant across these subpopulations (Figure 1). #Generate Data x &lt;- runif(500) yHomo &lt;- 2*x + rnorm(500) yHetero &lt;- 2*x + x*rnorm(500) df2 &lt;- as.data.frame(cbind(x, yHomo, yHetero)) #Scatter and Fitted Line ggplot(data=df2, aes(x=x, y=yHomo)) + geom_point() + geom_smooth(method=&#39;lm&#39;, se=F) + xlab(&quot;X variable&quot;) + theme_bw() Figure 1. Example of homoskedascity. Note how data points appear to be randomly scattered around line of best fit, and that the dispersion appears of the points constant across the range of X variable. #Scatter and Fitted Line ggplot(data=df2, aes(x=x, y=yHetero)) + geom_point() + geom_smooth(method=&#39;lm&#39;, se=F) + xlab(&quot;X variable&quot;) + theme_bw() Figure 2. Example of heteroskedascity. See how the dispersion of the points appears greater as X increases. Under the assumption of independent and identically distributed errors (homoskedascity), Eq. 3 is simplified to eq. 4 (transpose matrix, using diagonal elements). 3.5.1 Heteroskedascity in income data #Scatter and Fitted Line ggplot(data=df, aes(x=Income, y=Expenditure)) + geom_point() + geom_smooth(method=&#39;lm&#39;, formula=y ~ x + poly(x,2), se=F) + geom_smooth(method=&#39;lm&#39;, linetype=2, se=F) + xlab(&quot;Income&quot;) + theme_bw() ## Warning in predict.lm(model, newdata = data.frame(x = xseq), se.fit = se, : ## prediction from a rank-deficient fit may be misleading In our “real-world” small sample of data a visual representation of data can be challenging to draw conclusions from. We see there is an outlier (“Alaska”). However, it is difficult to judge overall dispersion with either a squared term [solid line] or a linear term [dashed line]. 3.6 “White” heteroskedastic consistent errors In the setting of heteroskedascity, the parameters are consistent but inefficient and also the variance-covariance matrix is inconsistent (i.e. biased).(White 1980) The assumption of the residuals \\(u\\) being identically distributed does not hold, and the diagonal matrix is invalid. However, an alternative variance-covariance matrix can be computed which is heteroskedastic consistent.(White 1980) With the “robust” approach proposed by White et al., you assume the variance of the residual is estimated as a diagonal matrix of each squared residual (vs. average above with \\(s^2\\)). Each j-th row-column element is \\(\\hat{u}_{j}^{2}\\) in the diagonal terms of \\({\\Sigma_{u}}\\). The full equation is: 3.6.1 Manual estimator u &lt;- matrix(resid(m1)) # residuals from model object meat1 &lt;- t(X) %*% diag(diag(crossprod(t(u)))) %*% X # Sigma is a diagonal with u^2 as elements dfc &lt;- n/(n-k) # degrees of freedom adjust se &lt;- sqrt(dfc*diag(solve(crossprod(X)) %*% meat1 %*% solve(crossprod(X)))) se_results[3,4] &lt;- se[2] se_results[3,6] &lt;- se[3] se_results ## # A tibble: 4 × 6 ## method `vcov Matrix` Income_Beta Income_SE Income2_Beta Income2_SE ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 manual iid -1834.203 828.9855 1587.042 519.0768 ## 2 lm iid -1834.203 828.9855 1587.042 519.0768 ## 3 manual White NA 1282.1010 NA 856.0721 ## 4 HC0 White (dfc) NA NA NA NA You will find these “White” or robust standard errors are consistent with the second Peterson table.(Petersen 2009) They are also consistent with STATA’s robust option. It is not technically the same as the White paper because STATA does a degree of freedom adjustment for small sample size. 3.6.2 R standard function Using the already written commands you can specify “White” standard errors with the vcovHC function in the sandwich package.(Zeileis 2006) You can report correct standard errors like below with vcovHC option in function coeftest. vcovHC has several types available. The general formula for the var-cov matrix is: \\((X&#39;X)^{-1} X&#39; Omega X (X&#39;X)^{-1}\\). The specification of \\(Omega\\) is determined by the type= option. type=&quot;cons&quot; \\(\\omega_i = \\sigma^2\\) Constant variance type=HC0 \\(\\omega_i = \\mu^2_i\\) the White variance-covariance matrix type=HC1 \\(\\omega_i = \\frac{n}{n-k}\\mu^2_i\\) Small sample correction (STATA). type=HC2 \\(\\omega_i = \\frac{\\mu^2_i}{1-h_i}\\) type=HC3 \\(\\omega_i = \\frac{\\mu^2_i}{(1-h_i)^{2}}\\) type=HC4 \\(\\omega_i = \\frac{\\mu^2_i}{(1-h_i)^{\\delta_i}}\\) Where \\(h_i = H_{ii}\\) are the diagonal elements of the hat matrix and \\(\\delta_i = min({4 }, {h_i}{h¯})\\). The documentation for the sandwich package recommends HC4 based on recent literature.(“Cribari-Neto F” 2004) ## Different variance-covariance options with vcovHC ## type = cons ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 832.91 327.29 2.5449 0.014275 * ## Income -1834.20 828.99 -2.2126 0.031820 * ## Income2 1587.04 519.08 3.0574 0.003677 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## type = HC0 ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 832.91 460.89 1.8072 0.07714 . ## Income -1834.20 1243.04 -1.4756 0.14673 ## Income2 1587.04 829.99 1.9121 0.06197 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## type = HC1 ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 832.91 475.37 1.7521 0.08627 . ## Income -1834.20 1282.10 -1.4306 0.15915 ## Income2 1587.04 856.07 1.8539 0.07004 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## type = HC2, ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 832.91 688.48 1.2098 0.2324 ## Income -1834.20 1866.41 -0.9827 0.3308 ## Income2 1587.04 1250.15 1.2695 0.2105 ## type = HC3, ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 832.91 1095.00 0.7607 0.4507 ## Income -1834.20 2975.41 -0.6165 0.5406 ## Income2 1587.04 1995.24 0.7954 0.4304 ## type = HC4, ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 832.91 3008.01 0.2769 0.7831 ## Income -1834.20 8183.19 -0.2241 0.8236 ## Income2 1587.04 5488.93 0.2891 0.7737 Lifehack: Rather than use the coeftest function you can also directly modify the standard errors in the regression summary object. s &lt;- summary(m1) s$coefficients[, 2] &lt;- sqrt(diag(vcovHC(m1, type=&quot;HC1&quot;))) s ## ## Call: ## lm(formula = Expenditure ~ Income + Income2, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -160.709 -36.896 -4.551 37.290 109.729 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 832.9 475.4 2.545 0.01428 * ## Income -1834.2 1282.1 -2.213 0.03182 * ## Income2 1587.0 856.1 3.057 0.00368 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 56.68 on 47 degrees of freedom ## Multiple R-squared: 0.6553, Adjusted R-squared: 0.6407 ## F-statistic: 44.68 on 2 and 47 DF, p-value: 1.345e-11 3.7 Acknowledgements This chapter is heavily adapted from several StackExchange and other blog posts. See: http://www.richard-bluhm.com/clustered-ses-in-r-and-stata-2/ https://sites.google.com/site/waynelinchang/r-code https://thetarzan.wordpress.com/2011/05/28/heteroskedasticity-robust-and-clustered-standard-errors-in-r/ 3.8 Bibliography References "],
["cluster-robust-standard-errors.html", "Chapter 4 Cluster Robust Standard Errors 4.1 Introduction 4.2 Clustering 4.3 Cluster robust errors in R 4.4 Block bootstrapping 4.5 Permutation or “Randomization” Test 4.6 Stata comparison 4.7 Acknowledgements", " Chapter 4 Cluster Robust Standard Errors 4.1 Introduction In this chapter we are evaluating R’s capability to compute different kinds of standard errors. Like with many things, R has extensive flexibility here but can be daunting when you want a quick option. To bring this flexibility down to earth, I lay out the background, provide practical recommendations, user-written commands and benchmark to STATA. 4.1.1 Packages to use R version 3.3.2 (2016-10-31) Platform: x86_64-w64-mingw32/x64 (64-bit) Running under: Windows 10 x64 (build 14393) attached base packages: [1] stats graphics grDevices utils datasets base other attached packages: [1] Scotty_0.0.0.9000 Hmisc_4.0-2 Formula_1.2-1 [4] survival_2.41-3 lattice_0.20-34 dplyr_0.5.0 [7] purrr_0.2.2 readr_1.1.0 tidyr_0.6.1 [10] tibble_1.3.0 ggplot2_2.2.1 tidyverse_1.1.1 “Scotty” is my own package. “tidyverse” is Wickam et al. general suite of packages/commands to work with R. “Hmisc” is Frank Harrel’s miscellaneous commands, many of which are quite useful. 4.2 Clustering 4.3 Cluster robust errors in R 4.4 Block bootstrapping An alternative to computing a special variance-covariance matrix is using a non-parametric “brute-force” method termed block bootstrapping. To do this, you the sample the dataset with replacement by group or “block” instead of individual observation. The parameters are estimated for each sample instance and stored in a new table. Then, you can either compute the parameter moments (mean, variance etc.) using the stored coefficients or if a 95% parameter interval is the ultimate goal one can simply report the ordered percentiles (e.g., 2.5% - 97.5%). Other methods for computing the intervals exist, such as bias-corrected. Whichever you pick, bootstraps are about as unbiased as the above sandwich estimators, and may be advantageous when the number of clusters is small. 4.5 Permutation or “Randomization” Test 4.5.1 Bootstrap Program Boot.ATE &lt;- function (model, treat, R = 250, block = &quot;&quot;, df) { require(boot) require(dplyr) family &lt;- model$family if (block == &quot;&quot;) { boot.mod &lt;- function(x, i, model, treat) { samp.df &lt;- x[i, ] samp.glm &lt;- try(glm(model, data = samp.df, family = family)) if (inherits(samp.glm, &quot;try-error&quot;)) { coef &lt;- NA ate &lt;- NA rr &lt;- NA c(coef, ate, rr) } else { df2 &lt;- samp.df df2[, paste(treat)] = 1 pred1. &lt;- predict.glm(samp.glm, newdata = df2, type = &quot;response&quot;) df2[, paste(treat)] = 0 pred0. &lt;- predict.glm(samp.glm, newdata = df2, type = &quot;response&quot;) coef &lt;- samp.glm$coefficients[paste0(treat)] ate &lt;- mean(pred1.) - mean(pred0.) rr &lt;- mean(pred1.)/mean(pred0.) c(coef, ate, rr) } } boot.m &lt;- boot(data = df, statistic = boot.mod, R = R, model = model, treat = treat) } else { Groups = unique(df[, paste(block)]) boot.mod &lt;- function(x, i, model, treat, df, block, iter = 0) { block.df &lt;- data.frame(group = x[i]) names(block.df) = block samp.df &lt;- left_join(block.df, df, by = block) samp.glm &lt;- try(glm(model, data = samp.df, family = family)) if (inherits(samp.glm, &quot;try-error&quot;)) { coef &lt;- NA ate &lt;- NA rr &lt;- NA c(coef, ate, rr) } else { df2 &lt;- samp.df df2[, paste(treat)] = 1 pred1. &lt;- predict.glm(samp.glm, newdata = df2, type = &quot;response&quot;) df2[, paste(treat)] = 0 pred0. &lt;- predict.glm(samp.glm, newdata = df2, type = &quot;response&quot;) coef &lt;- samp.glm$coefficients[paste0(treat)] ate &lt;- mean(pred1.) - mean(pred0.) rr &lt;- mean(pred1.)/mean(pred0.) c(coef, ate, rr) } } boot.m &lt;- boot(data = Groups, statistic = boot.mod, R = R, model = model, treat = treat, df = df, block = block) } m1.confint &lt;- c(model$coefficients[paste0(treat)], confint(model, treat, level = 0.95)) coeff = boot.ci(boot.m, index = 1, type = &quot;perc&quot;) coeff = c(median(boot.m$t[, 1]), coeff$percent[, 4], coeff$percent[, 5]) names(coeff) &lt;- c(&quot;Coeff.&quot;, &quot;2.5%&quot;, &quot;97.5%&quot;) ate = boot.ci(boot.m, index = 2, type = &quot;perc&quot;) ate = c(median(boot.m$t[, 2]), ate$percent[, 4], ate$percent[, 5]) names(ate) &lt;- c(&quot;ATE&quot;, &quot;2.5%&quot;, &quot;97.5%&quot;) rr = boot.ci(boot.m, index = 3, type = &quot;perc&quot;) rr = c(median(boot.m$t[, 3]), rr$percent[, 4], rr$percent[, 5]) names(rr) &lt;- c(&quot;Rr&quot;, &quot;2.5%&quot;, &quot;97.5%&quot;) boot.iter = boot.m$t res = list(level = 0.95, model_ci = m1.confint, coeff = coeff, ate = ate, rr = rr, boots = boot.iter) return(res) ############################################## ## Testing coefficients in time-series data ## ############################################## ## Load investment equation data: data(Investment) # Fit regression model: fm.inv &lt;- lm(RealInv ~ RealGNP + RealInt, data = Investment) ## Test coefficients using Newey-West HAC estimator with ## user-defined and data-driven bandwidth and with Parzen kernel: coeftest(fm.inv, df = Inf, vcov = NeweyWest(fm.inv, lag = 4, prewhite = FALSE)) coeftest(fm.inv, df = Inf, vcov = NeweyWest) parzenHAC &lt;- function(x, ...) kernHAC(x, kernel = &quot;Parzen&quot;, prewhite = 2, adjust = FALSE, bw = bwNeweyWest, ...) coeftest(fm.inv, df = Inf, vcov = parzenHAC) ## Time-series visualization: plot(Investment[, &quot;RealInv&quot;], type = &quot;b&quot;, pch = 19, ylab = &quot;Real investment&quot;) lines(ts(fitted(fm.inv), start = 1964), col = 4) ## 3-dimensional visualization: library(scatterplot3d) s3d &lt;- scatterplot3d(Investment[,c(5,7,6)], type = &quot;b&quot;, angle = 65, scale.y = 1, pch = 16) s3d$plane3d(fm.inv, lty.box = &quot;solid&quot;, col = 4) ########################################################### ## Testing and dating structural changes in the presence ## ## of heteroskedasticity and autocorrelation ## ########################################################### ## Load real interest series: data(RealInt) ## OLS-based CUSUM test with quadratic spectral kernel HAC estimate: ocus &lt;- gefp(RealInt ~ 1, fit = lm, vcov = kernHAC) plot(ocus, aggregate = FALSE) sctest(ocus) ## supF test with quadratic spectral kernel HAC estimate: fs &lt;- Fstats(RealInt ~ 1, vcov = kernHAC) plot(fs) sctest(fs) ## Breakpoint estimation and confidence intervals ## with quadratic spectral kernel HAC estimate: bp &lt;- breakpoints(RealInt ~ 1) confint(bp, vcov = kernHAC) plot(bp) ## Visualization: plot(RealInt, ylab = &quot;Real interest rate&quot;) lines(ts(fitted(bp), start = start(RealInt), freq = 4), col = 4) lines(confint(bp, vcov = kernHAC)) } 4.6 Stata comparison A full discussion of STATA programming can be seen here: http://www.kellogg.northwestern.edu/faculty/petersen/htm/papers/se/se_programming.htm STATA blog: http://www.stata.com/support/faqs/statistics/standard-errors-and-vce-cluster-option/ Briefly: In Stata one can specify a variance-covariance matrix that is heteroskedastic consistent with the vce(robust) option in regression models. e.g. robust option in STATA regress y x, vce(robust) A Huber-White variance-covariance matrix can also be computed by some group with the vce(cluster group) option in regression models. e.g. cluster option in STATA regress y x, vce(cluster group) See: http://www.stata.com/support/faqs/statistics/standard-errors-and-vce-cluster-option/ 4.7 Acknowledgements This chapter is heavily adapted from several StackExchange and other blog posts. See: http://www.richard-bluhm.com/clustered-ses-in-r-and-stata-2/ https://sites.google.com/site/waynelinchang/r-code https://thetarzan.wordpress.com/2011/05/28/heteroskedasticity-robust-and-clustered-standard-errors-in-r/ "]
]
